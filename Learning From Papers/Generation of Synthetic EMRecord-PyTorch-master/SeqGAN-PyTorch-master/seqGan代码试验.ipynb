{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator \"\"\"\n",
    "    def __init__(self, num_emb, emb_dim, hidden_dim, use_cuda):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_emb = num_emb\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_cuda = use_cuda\n",
    "        self.emb = nn.Embedding(num_emb, emb_dim) #num_emb嵌入词典的大小；emb_dim每个嵌入向量的维度；\n",
    "        # When batch_first is True, then the input and output tensors are provided as (batch, seq, feature).\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True) # 为什么不用GRU\n",
    "        self.lin = nn.Linear(hidden_dim, num_emb)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len), sequence of tokens generated by generator\n",
    "        \"\"\"\n",
    "        emb = self.emb(x)\n",
    "        h0, c0 = self.init_hidden(x.size(0))\n",
    "        output, (h, c) = self.lstm(emb, (h0, c0)) # output:(batch_size,seq_len,hidden_dim)\n",
    "        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim))) #这里不要声明dim吗？\n",
    "        return pred\n",
    "\n",
    "    def step(self, x, h, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size,  1), sequence of tokens generated by generator\n",
    "            h: (1, batch_size, hidden_dim), lstm hidden state\n",
    "            c: (1, batch_size, hidden_dim), lstm cell state\n",
    "        \"\"\"\n",
    "        emb = self.emb(x)\n",
    "        output, (h, c) = self.lstm(emb, (h, c))\n",
    "        pred = F.softmax(self.lin(output.view(-1, self.hidden_dim)), dim=1)\n",
    "        return pred, h, c\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        c = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        if self.use_cuda:\n",
    "            h, c = h.cuda(), c.cuda()\n",
    "        return h, c\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "    def sample(self, batch_size, seq_len, x=None):\n",
    "        res = []\n",
    "        flag = False # whether sample from zero\n",
    "        if x is None:\n",
    "            flag = True\n",
    "        if flag:\n",
    "            x = Variable(torch.zeros((batch_size, 1)).long())\n",
    "        if self.use_cuda:\n",
    "            x = x.cuda()\n",
    "        h, c = self.init_hidden(batch_size)\n",
    "        samples = []\n",
    "        if flag:\n",
    "            for i in range(seq_len):\n",
    "                output, h, c = self.step(x, h, c)\n",
    "                x = output.multinomial(1)\n",
    "                samples.append(x)\n",
    "        else:\n",
    "            given_len = x.size(1)\n",
    "            lis = x.chunk(x.size(1), dim=1)\n",
    "            for i in range(given_len):\n",
    "                output, h, c = self.step(lis[i], h, c)\n",
    "                samples.append(lis[i])\n",
    "            x = output.multinomial(1)\n",
    "            for i in range(given_len, seq_len):\n",
    "                samples.append(x)\n",
    "                output, h, c = self.step(x, h, c)\n",
    "                x = output.multinomial(1)\n",
    "        output = torch.cat(samples, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_emb = 5000,emb_dim = 128,hidden_dim = 64,use_cuda = 'Ture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (emb): Embedding(5000, 128)\n",
       "  (lstm): LSTM(128, 64, batch_first=True)\n",
       "  (lin): Linear(in_features=64, out_features=5000, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.LongTensor([[2,50,100],\n",
    "                       [40,3,1000]]).cuda()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "emb:(2,3,128)\n",
    "output:(2,3,64)\n",
    "pred:(6,5000)\n",
    "'''\n",
    "pred = generator(x) # 我的预测结构应该是（6， 128）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(generator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.weight : torch.Size([5000, 128])\n",
      "lstm.weight_ih_l0 : torch.Size([256, 128])\n",
      "lstm.weight_hh_l0 : torch.Size([256, 64])\n",
      "lstm.bias_ih_l0 : torch.Size([256])\n",
      "lstm.bias_hh_l0 : torch.Size([256])\n",
      "lin.weight : torch.Size([5000, 64])\n",
      "lin.bias : torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in generator.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# discriminator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "d_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "d_dropout = 0.75\n",
    "d_emb_dim = 64\n",
    "d_num_class = 2\n",
    "VOCAB_SIZE = 5000\n",
    "# discriminator = Discriminator(num_classes, vocab_size, emb_dim, filter_sizes, num_filters, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_filter_sizes),len(d_num_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1 , f = 100 \n",
      "\n",
      "n = 2 , f = 200 \n",
      "\n",
      "n = 3 , f = 200 \n",
      "\n",
      "n = 4 , f = 200 \n",
      "\n",
      "n = 5 , f = 200 \n",
      "\n",
      "n = 6 , f = 100 \n",
      "\n",
      "n = 7 , f = 100 \n",
      "\n",
      "n = 8 , f = 100 \n",
      "\n",
      "n = 9 , f = 100 \n",
      "\n",
      "n = 10 , f = 100 \n",
      "\n",
      "n = 15 , f = 160 \n",
      "\n",
      "n = 20 , f = 160 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (n,f) in zip(d_filter_sizes,d_num_filters):\n",
    "    print('n = %d , f = %d \\n'%(n,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"A CNN for text classification\n",
    "\n",
    "    architecture: Embedding >> Convolution >> Max-pooling >> Softmax\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, vocab_size, emb_dim, filter_sizes, num_filters, dropout):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, emb_dim)) for (n, f) in zip(num_filters, filter_sizes)\n",
    "        ])\n",
    "        self.highway = nn.Linear(sum(num_filters), sum(num_filters))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lin = nn.Linear(sum(num_filters), num_classes)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.init_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size * seq_len)\n",
    "        \"\"\"\n",
    "        emb = self.emb(x).unsqueeze(1)  # batch_size * 1 * seq_len * emb_dim\n",
    "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]\n",
    "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs] # [batch_size * num_filter]\n",
    "        pred = torch.cat(pools, 1)  # batch_size * num_filters_sum\n",
    "        highway = self.highway(pred)\n",
    "        pred = torch.sigmoid(highway) *  F.relu(highway) + (1. - torch.sigmoid(highway)) * pred\n",
    "        pred = self.softmax(self.lin(self.dropout(pred)))\n",
    "        return pred\n",
    "\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.05, 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator(d_num_class, VOCAB_SIZE, d_emb_dim, d_filter_sizes, d_num_filters, d_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (emb): Embedding(5000, 64)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(1, 64), stride=(1, 1))\n",
       "    (1): Conv2d(1, 200, kernel_size=(2, 64), stride=(1, 1))\n",
       "    (2): Conv2d(1, 200, kernel_size=(3, 64), stride=(1, 1))\n",
       "    (3): Conv2d(1, 200, kernel_size=(4, 64), stride=(1, 1))\n",
       "    (4): Conv2d(1, 200, kernel_size=(5, 64), stride=(1, 1))\n",
       "    (5): Conv2d(1, 100, kernel_size=(6, 64), stride=(1, 1))\n",
       "    (6): Conv2d(1, 100, kernel_size=(7, 64), stride=(1, 1))\n",
       "    (7): Conv2d(1, 100, kernel_size=(8, 64), stride=(1, 1))\n",
       "    (8): Conv2d(1, 100, kernel_size=(9, 64), stride=(1, 1))\n",
       "    (9): Conv2d(1, 100, kernel_size=(10, 64), stride=(1, 1))\n",
       "    (10): Conv2d(1, 160, kernel_size=(15, 64), stride=(1, 1))\n",
       "    (11): Conv2d(1, 160, kernel_size=(20, 64), stride=(1, 1))\n",
       "  )\n",
       "  (highway): Linear(in_features=1720, out_features=1720, bias=True)\n",
       "  (dropout): Dropout(p=0.75)\n",
       "  (lin): Linear(in_features=1720, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(100,30,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 30, 40])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = F.max_pool1d(a,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 30, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "emb_dim = 20\n",
    "hidden_dim = 20\n",
    "\n",
    "h_0 = torch.randn(1,batch_size,hidden_dim)\n",
    "c_0 = torch.randn(1,batch_size,hidden_dim)\n",
    "x = torch.zeros(batch_size,1).long()\n",
    "emb = nn.Embedding(50,emb_dim)\n",
    "y = emb(x)\n",
    "lstm = nn.LSTM(emb_dim,hidden_dim,batch_first = True)\n",
    "output,(h,c) = lstm(y,(h_0,c_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9810e-01, -1.2066e-01,  1.6467e-01,  1.0540e-01,  6.5125e-02,\n",
       "          -3.8412e-01, -9.1172e-02,  9.3457e-02,  2.0005e-01, -1.2519e-01,\n",
       "          -4.4131e-02, -6.6144e-02, -3.0389e-01,  6.8024e-02,  1.2933e-01,\n",
       "           1.1670e-01,  6.7173e-02,  6.2757e-01,  1.8349e-01, -2.5825e-01]],\n",
       "\n",
       "        [[ 1.3263e-01,  4.0234e-01,  2.3037e-01, -2.4626e-01, -2.9471e-01,\n",
       "           4.0388e-01,  1.6501e-01, -2.8089e-01,  9.1824e-02, -1.8406e-02,\n",
       "          -3.0841e-02, -2.3087e-01, -7.2299e-01, -3.6598e-02,  8.1172e-02,\n",
       "          -1.2660e-01,  3.3301e-02, -3.2666e-01, -2.0666e-02, -1.4617e-01]],\n",
       "\n",
       "        [[-3.4644e-01,  2.9565e-01,  3.1386e-01, -7.9993e-02,  9.6691e-02,\n",
       "          -6.4538e-02,  1.2887e-01,  3.3819e-01, -5.2917e-02,  2.5142e-01,\n",
       "          -1.2581e-01, -9.9608e-02,  6.6515e-02,  1.7201e-02,  6.2637e-02,\n",
       "           8.6001e-02,  1.7932e-01, -1.2206e-02,  1.5722e-02, -4.2225e-01]],\n",
       "\n",
       "        [[-3.9014e-01,  8.3860e-02,  4.9311e-01,  1.3096e-02, -1.7987e-01,\n",
       "           4.2378e-01,  3.0578e-01,  1.4955e-01,  1.4267e-01,  1.4585e-01,\n",
       "           1.5881e-01, -3.4437e-01,  4.6173e-01,  2.1746e-02,  3.8692e-01,\n",
       "           1.1513e-02, -4.9986e-01,  2.4231e-01,  5.4142e-01,  1.6973e-01]],\n",
       "\n",
       "        [[-2.8100e-01, -1.0934e-01, -1.2308e-01, -1.7163e-01, -1.1448e-01,\n",
       "          -3.1749e-01, -4.8799e-01, -2.3768e-01,  2.6102e-01,  5.5894e-01,\n",
       "           2.5378e-01, -1.0140e-01, -4.2180e-01,  4.7480e-02, -2.2051e-01,\n",
       "          -1.6128e-01,  5.6289e-02,  6.1807e-01,  4.3150e-01, -3.5150e-01]],\n",
       "\n",
       "        [[ 2.6491e-01, -5.4398e-02,  2.6767e-01,  8.9460e-02,  2.0910e-02,\n",
       "          -1.6198e-01, -1.1108e-01,  1.1571e-01, -1.2435e-01, -9.4490e-02,\n",
       "          -2.0753e-01,  1.5483e-01, -6.1957e-01, -6.4556e-02,  2.4597e-01,\n",
       "           1.9981e-01,  4.4651e-03,  5.0215e-01, -2.2765e-01, -3.1235e-01]],\n",
       "\n",
       "        [[-2.3756e-01,  2.5525e-01,  2.9850e-01,  3.8247e-02, -4.2310e-01,\n",
       "          -2.2904e-01,  1.3636e-01, -3.7091e-01,  1.9810e-01,  2.0717e-01,\n",
       "           5.6232e-02, -1.0379e-01, -6.6876e-01,  2.0890e-01,  2.3795e-01,\n",
       "          -9.1879e-02, -2.9690e-01, -8.4608e-02,  4.8385e-01, -2.3179e-01]],\n",
       "\n",
       "        [[-9.8428e-02,  3.3169e-01,  5.1149e-01, -3.4554e-01, -1.6935e-01,\n",
       "          -3.8681e-01,  1.9684e-01,  1.9491e-01,  1.3824e-01,  2.1413e-01,\n",
       "          -2.7517e-01, -1.8814e-03, -4.4634e-01, -2.3539e-01,  4.6462e-01,\n",
       "          -2.6287e-01,  8.1657e-02,  3.5136e-01,  4.4682e-02, -1.9132e-01]],\n",
       "\n",
       "        [[-3.9659e-01,  1.3181e-01,  1.9353e-01, -5.9649e-03, -4.4787e-01,\n",
       "           4.1640e-01, -1.4395e-01, -4.8233e-01,  1.9052e-01, -9.8235e-02,\n",
       "          -3.3024e-02,  1.7723e-01, -1.7087e-01,  2.9815e-02, -4.2425e-02,\n",
       "           5.9657e-02, -5.4679e-01, -2.0092e-01, -9.2560e-02, -2.9656e-01]],\n",
       "\n",
       "        [[-1.9451e-01,  2.5994e-01,  2.5411e-01,  4.0537e-02, -2.7679e-01,\n",
       "          -9.9727e-02, -1.7861e-01,  8.6510e-02,  2.7238e-01,  1.9787e-01,\n",
       "           4.6472e-02,  5.3844e-02, -1.3412e-01, -3.1923e-01, -2.7755e-01,\n",
       "          -5.6753e-02, -2.8276e-01,  1.8736e-01,  4.7077e-01, -5.0208e-01]],\n",
       "\n",
       "        [[ 3.7127e-01, -1.2129e-01, -4.5257e-01,  1.0524e-01, -5.9840e-02,\n",
       "          -1.4141e-01,  2.5451e-01,  5.8261e-02,  1.7102e-01, -3.5991e-02,\n",
       "           1.6785e-01, -2.5494e-01, -5.9773e-01,  1.6168e-01,  3.9811e-01,\n",
       "           5.8796e-02, -3.3387e-01, -5.8472e-01,  5.6853e-01, -4.3997e-01]],\n",
       "\n",
       "        [[-3.6151e-02, -1.9657e-01,  3.8265e-01, -1.0234e-01, -5.7423e-01,\n",
       "           1.8340e-01, -1.9423e-01,  2.9122e-01,  1.7406e-01,  5.7467e-02,\n",
       "          -2.3190e-03,  2.5344e-01,  1.1938e-01,  2.2982e-01,  1.8128e-01,\n",
       "           4.6979e-02, -2.0221e-01, -4.0566e-01,  1.7168e-01, -1.2786e-01]],\n",
       "\n",
       "        [[ 3.9426e-01,  3.3191e-01,  5.2302e-01, -1.6665e-01, -3.9263e-01,\n",
       "          -8.9565e-02, -9.8342e-02,  2.6198e-01,  1.1132e-01,  5.4855e-02,\n",
       "          -3.5591e-02,  1.1721e-01, -7.5421e-01,  1.0674e-01,  2.4556e-02,\n",
       "          -1.8583e-01, -2.9456e-01,  5.7465e-01,  3.0848e-01,  9.6581e-02]],\n",
       "\n",
       "        [[-2.0708e-01,  1.5794e-01,  6.3958e-01,  3.4692e-02, -1.6997e-01,\n",
       "           3.1890e-01, -6.6124e-02,  1.7575e-01, -2.0032e-02,  7.2667e-03,\n",
       "          -9.5074e-02,  6.3021e-02,  4.2284e-01,  8.7883e-02, -5.9459e-02,\n",
       "           3.4590e-03,  2.6827e-02, -4.6532e-01,  9.6996e-02, -5.6544e-02]],\n",
       "\n",
       "        [[ 2.4102e-01,  5.2562e-01,  3.9661e-01,  1.9018e-01,  4.9000e-01,\n",
       "          -4.4441e-01, -3.4493e-01,  2.6790e-01,  2.2993e-01,  6.2463e-02,\n",
       "           1.2692e-01, -7.1214e-02, -5.0597e-01,  1.0967e-01,  3.6974e-01,\n",
       "          -1.0273e-01, -5.3602e-01,  5.0244e-01,  3.5946e-01, -3.1976e-01]],\n",
       "\n",
       "        [[-2.2171e-01,  2.0912e-01,  2.4965e-02,  5.3695e-02, -3.6174e-01,\n",
       "           5.0949e-01, -1.9895e-01,  1.7894e-01,  2.3754e-01, -6.5599e-02,\n",
       "          -1.7490e-01, -4.4418e-01,  3.4656e-01,  2.1936e-01,  1.5258e-01,\n",
       "           4.6081e-01, -4.2674e-01,  3.9335e-01,  1.0990e-01, -1.2230e-01]],\n",
       "\n",
       "        [[-4.6926e-02,  4.6309e-01,  2.9751e-01,  2.3641e-02, -1.8612e-02,\n",
       "          -3.4062e-01,  3.3667e-02,  1.7918e-01,  9.5288e-02, -3.7474e-01,\n",
       "          -1.2580e-01, -2.6483e-01, -1.8451e-01,  1.8873e-01,  9.2098e-02,\n",
       "          -3.8526e-02, -1.3412e-01, -2.0875e-01,  2.2794e-01, -4.0097e-01]],\n",
       "\n",
       "        [[ 2.5458e-01,  7.5675e-02,  1.9506e-01, -1.5096e-01,  3.7456e-01,\n",
       "          -7.5122e-02, -1.8054e-02,  3.2001e-01,  9.3946e-02,  8.7564e-02,\n",
       "           2.4225e-02,  4.2710e-02,  1.9293e-01,  1.8594e-02, -8.4978e-02,\n",
       "          -2.2283e-01, -3.1039e-01, -4.8015e-01, -1.7812e-02, -2.3318e-01]],\n",
       "\n",
       "        [[-6.2345e-02,  8.0012e-02,  5.5400e-01, -7.6434e-02,  3.0062e-01,\n",
       "           3.5480e-02,  8.1962e-02,  7.6111e-02,  6.6327e-02,  1.8297e-01,\n",
       "           6.9780e-02, -1.0445e-01, -3.2048e-01,  1.2401e-01, -1.1550e-01,\n",
       "           8.4059e-02,  2.2038e-02,  2.4746e-01,  2.8093e-01,  9.4333e-02]],\n",
       "\n",
       "        [[-2.7266e-01,  2.1560e-01,  5.7128e-01,  7.8909e-02,  5.7735e-02,\n",
       "          -1.0304e-01, -2.6066e-01,  2.6943e-02,  1.5446e-01, -9.9981e-02,\n",
       "           1.4321e-01, -6.7076e-02,  2.8803e-01,  2.3873e-01,  1.9099e-02,\n",
       "          -2.1464e-01, -4.3942e-01,  2.1304e-01,  4.1908e-01,  1.5337e-03]],\n",
       "\n",
       "        [[ 1.3233e-01,  1.9335e-01,  2.1658e-01, -4.2555e-02, -3.8107e-01,\n",
       "          -1.0285e-01,  5.6006e-03, -2.6975e-01,  1.5003e-02,  8.4192e-02,\n",
       "          -1.7026e-01, -1.3742e-01,  3.0212e-01, -8.7590e-03,  3.0386e-01,\n",
       "          -1.7167e-01, -7.0291e-02,  6.4868e-01,  9.6476e-02, -1.6222e-01]],\n",
       "\n",
       "        [[-6.7175e-01, -1.5215e-01,  1.6189e-01,  7.3558e-03,  1.4674e-03,\n",
       "          -6.0023e-02,  1.3995e-01,  1.4467e-01,  1.5790e-01,  5.3176e-01,\n",
       "          -3.6495e-02,  1.9776e-01,  4.1944e-01,  3.4398e-01,  3.0334e-01,\n",
       "          -1.6499e-01, -3.0245e-01,  1.2621e-01,  1.9051e-01,  3.5931e-01]],\n",
       "\n",
       "        [[-1.3746e-01,  2.5308e-01,  3.7122e-01, -8.0363e-02,  6.4163e-02,\n",
       "           4.2276e-01, -2.4097e-01,  1.6449e-01,  7.9621e-02,  2.1879e-01,\n",
       "           4.0292e-02,  2.0105e-01, -3.2234e-01,  3.7364e-01,  4.7176e-01,\n",
       "           2.0840e-01, -1.4342e-01, -3.0287e-01,  4.0022e-01,  1.6501e-01]],\n",
       "\n",
       "        [[-2.8311e-01,  1.0586e-01,  6.0980e-01, -1.3772e-01,  2.2145e-01,\n",
       "          -2.3077e-01,  7.2824e-02,  2.4933e-01,  1.8257e-01, -3.3953e-01,\n",
       "           3.6809e-02, -6.3701e-02, -2.5666e-01,  1.3246e-01,  1.9114e-01,\n",
       "          -3.4028e-01, -5.8572e-02, -4.6636e-01,  3.3169e-01,  2.3928e-02]],\n",
       "\n",
       "        [[-1.7224e-01,  1.3854e-01,  1.5816e-01, -4.3887e-02,  6.9626e-02,\n",
       "          -1.4465e-01,  1.0544e-01, -4.8917e-02,  1.9189e-01,  2.2744e-04,\n",
       "           1.9446e-01, -1.5704e-01, -6.2857e-01, -1.6370e-01,  2.2053e-01,\n",
       "           3.5936e-02, -4.3425e-01, -2.6217e-01, -5.0350e-02, -2.5415e-01]],\n",
       "\n",
       "        [[ 7.6116e-02,  2.5313e-01,  3.3748e-01,  2.4972e-01,  6.4913e-02,\n",
       "          -4.6175e-01, -4.2802e-01, -4.0231e-01,  2.6855e-01, -1.2328e-01,\n",
       "          -1.4087e-01, -4.2831e-01, -1.9050e-02, -7.0036e-02,  3.2709e-01,\n",
       "          -1.3892e-01, -2.5468e-01,  1.8793e-01,  5.7546e-02, -3.5298e-01]],\n",
       "\n",
       "        [[-5.5940e-01,  3.1031e-01,  2.6904e-01,  1.1461e-01, -1.0042e-01,\n",
       "          -4.7655e-01,  4.9409e-02,  1.0221e-01,  2.4822e-01, -8.2271e-02,\n",
       "          -8.5915e-02,  3.0802e-02, -3.4983e-01, -1.8766e-01, -2.4135e-01,\n",
       "          -1.0811e-01, -6.6660e-02, -6.1317e-01,  1.3300e-01, -1.4731e-01]],\n",
       "\n",
       "        [[-3.8069e-01,  4.7813e-02, -7.9206e-02, -1.0673e-01, -1.1143e-01,\n",
       "           2.9490e-01, -3.8887e-02, -2.8309e-01,  1.4759e-01,  3.2132e-01,\n",
       "          -1.9039e-01, -2.4941e-01, -2.4150e-01,  6.7355e-02,  3.2307e-01,\n",
       "           1.4005e-01, -1.7914e-01,  3.4907e-03,  8.7618e-02, -9.6974e-02]],\n",
       "\n",
       "        [[-5.0169e-01,  3.0591e-01, -1.5514e-01, -2.3342e-02, -1.8319e-01,\n",
       "          -4.6530e-01, -2.0924e-01,  3.0429e-02,  1.3091e-01,  1.5345e-01,\n",
       "          -1.2941e-01,  3.1498e-01,  1.1710e-01,  7.7609e-02,  1.7251e-01,\n",
       "          -1.4443e-01, -5.6708e-01, -1.2012e-01,  1.1137e-01, -2.9216e-01]],\n",
       "\n",
       "        [[-4.3212e-01,  2.0219e-01,  3.2545e-01,  1.6905e-01,  1.4273e-01,\n",
       "          -7.7762e-02,  2.5315e-01, -8.5483e-02, -7.0903e-03,  1.9866e-01,\n",
       "          -4.2005e-02, -1.8369e-01,  6.1822e-02,  9.0297e-02,  1.3656e-02,\n",
       "          -7.4895e-02,  2.5240e-01,  2.1781e-01,  3.9933e-01, -1.7341e-01]],\n",
       "\n",
       "        [[-3.1046e-01,  6.4769e-02,  2.5634e-01, -1.2673e-01,  1.1352e-01,\n",
       "           2.5227e-01, -7.3130e-02, -6.9133e-04, -1.8313e-02,  4.0874e-02,\n",
       "          -1.7150e-01,  8.8992e-02, -6.7882e-01, -1.5984e-01,  2.4700e-01,\n",
       "          -4.8208e-02, -3.4094e-01,  5.9538e-01,  2.3309e-01, -2.1765e-01]],\n",
       "\n",
       "        [[-6.3397e-01,  2.1014e-01,  2.5447e-01,  2.9462e-01,  9.6536e-02,\n",
       "          -4.3273e-01,  8.6437e-02, -4.2575e-01,  1.5014e-01, -2.0834e-01,\n",
       "          -6.6861e-02,  2.7971e-01, -7.7787e-02, -4.6427e-02,  3.9994e-01,\n",
       "          -1.8152e-01, -3.4833e-01,  5.6377e-01,  4.9380e-01, -3.0045e-01]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: cannot sample n_sample > prob_dist.size(1) samples without replacement at ..\\aten\\src\\TH/generic/THTensorRandom.cpp:286",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-b9d341a1c4bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mz_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 2: cannot sample n_sample > prob_dist.size(1) samples without replacement at ..\\aten\\src\\TH/generic/THTensorRandom.cpp:286"
     ]
    }
   ],
   "source": [
    "z_1 = torch.exp(output)\n",
    "z = z_1.multinomial(1)\n",
    "k = z_1.multinomial(2)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8203, 0.8863, 1.1790, 1.1112, 1.0673, 0.6811, 0.9129, 1.0980,\n",
       "          1.2215, 0.8823, 0.9568, 0.9360, 0.7379, 1.0704, 1.1381, 1.1238,\n",
       "          1.0695, 1.8731, 1.2014, 0.7724]],\n",
       "\n",
       "        [[1.1418, 1.4953, 1.2591, 0.7817, 0.7447, 1.4976, 1.1794, 0.7551,\n",
       "          1.0962, 0.9818, 0.9696, 0.7938, 0.4853, 0.9641, 1.0846, 0.8811,\n",
       "          1.0339, 0.7213, 0.9795, 0.8640]],\n",
       "\n",
       "        [[0.7072, 1.3440, 1.3687, 0.9231, 1.1015, 0.9375, 1.1375, 1.4024,\n",
       "          0.9485, 1.2859, 0.8818, 0.9052, 1.0688, 1.0173, 1.0646, 1.0898,\n",
       "          1.1964, 0.9879, 1.0158, 0.6556]],\n",
       "\n",
       "        [[0.6770, 1.0875, 1.6374, 1.0132, 0.8354, 1.5277, 1.3577, 1.1613,\n",
       "          1.1533, 1.1570, 1.1721, 0.7087, 1.5868, 1.0220, 1.4724, 1.0116,\n",
       "          0.6066, 1.2742, 1.7185, 1.1850]],\n",
       "\n",
       "        [[0.7550, 0.8964, 0.8842, 0.8423, 0.8918, 0.7280, 0.6139, 0.7885,\n",
       "          1.2982, 1.7488, 1.2889, 0.9036, 0.6559, 1.0486, 0.8021, 0.8511,\n",
       "          1.0579, 1.8554, 1.5396, 0.7036]],\n",
       "\n",
       "        [[1.3033, 0.9471, 1.3069, 1.0936, 1.0211, 0.8505, 0.8949, 1.1227,\n",
       "          0.8831, 0.9098, 0.8126, 1.1675, 0.5382, 0.9375, 1.2789, 1.2212,\n",
       "          1.0045, 1.6523, 0.7964, 0.7317]],\n",
       "\n",
       "        [[0.7886, 1.2908, 1.3478, 1.0390, 0.6550, 0.7953, 1.1461, 0.6901,\n",
       "          1.2191, 1.2302, 1.0578, 0.9014, 0.5123, 1.2323, 1.2686, 0.9122,\n",
       "          0.7431, 0.9189, 1.6223, 0.7931]],\n",
       "\n",
       "        [[0.9063, 1.3933, 1.6678, 0.7078, 0.8442, 0.6792, 1.2175, 1.2152,\n",
       "          1.1483, 1.2388, 0.7594, 0.9981, 0.6400, 0.7903, 1.5914, 0.7688,\n",
       "          1.0851, 1.4210, 1.0457, 0.8259]],\n",
       "\n",
       "        [[0.6726, 1.1409, 1.2135, 0.9941, 0.6390, 1.5165, 0.8659, 0.6173,\n",
       "          1.2099, 0.9064, 0.9675, 1.1939, 0.8429, 1.0303, 0.9585, 1.0615,\n",
       "          0.5788, 0.8180, 0.9116, 0.7434]],\n",
       "\n",
       "        [[0.8232, 1.2969, 1.2893, 1.0414, 0.7582, 0.9051, 0.8364, 1.0904,\n",
       "          1.3131, 1.2188, 1.0476, 1.0553, 0.8745, 0.7267, 0.7576, 0.9448,\n",
       "          0.7537, 1.2061, 1.6012, 0.6053]],\n",
       "\n",
       "        [[1.4496, 0.8858, 0.6360, 1.1110, 0.9419, 0.8681, 1.2898, 1.0600,\n",
       "          1.1865, 0.9646, 1.1828, 0.7750, 0.5501, 1.1755, 1.4890, 1.0606,\n",
       "          0.7161, 0.5573, 1.7657, 0.6441]],\n",
       "\n",
       "        [[0.9645, 0.8215, 1.4662, 0.9027, 0.5631, 1.2013, 0.8235, 1.3381,\n",
       "          1.1901, 1.0592, 0.9977, 1.2885, 1.1268, 1.2584, 1.1988, 1.0481,\n",
       "          0.8169, 0.6665, 1.1873, 0.8800]],\n",
       "\n",
       "        [[1.4833, 1.3936, 1.6871, 0.8465, 0.6753, 0.9143, 0.9063, 1.2995,\n",
       "          1.1177, 1.0564, 0.9650, 1.1244, 0.4704, 1.1126, 1.0249, 0.8304,\n",
       "          0.7449, 1.7765, 1.3614, 1.1014]],\n",
       "\n",
       "        [[0.8130, 1.1711, 1.8957, 1.0353, 0.8437, 1.3756, 0.9360, 1.1921,\n",
       "          0.9802, 1.0073, 0.9093, 1.0650, 1.5263, 1.0919, 0.9423, 1.0035,\n",
       "          1.0272, 0.6279, 1.1019, 0.9450]],\n",
       "\n",
       "        [[1.2725, 1.6915, 1.4868, 1.2095, 1.6323, 0.6412, 0.7083, 1.3072,\n",
       "          1.2585, 1.0645, 1.1353, 0.9313, 0.6029, 1.1159, 1.4474, 0.9024,\n",
       "          0.5851, 1.6528, 1.4326, 0.7263]],\n",
       "\n",
       "        [[0.8011, 1.2326, 1.0253, 1.0552, 0.6965, 1.6644, 0.8196, 1.1960,\n",
       "          1.2681, 0.9365, 0.8395, 0.6413, 1.4142, 1.2453, 1.1648, 1.5854,\n",
       "          0.6526, 1.4819, 1.1162, 0.8849]],\n",
       "\n",
       "        [[0.9542, 1.5890, 1.3465, 1.0239, 0.9816, 0.7113, 1.0342, 1.1962,\n",
       "          1.1000, 0.6875, 0.8818, 0.7673, 0.8315, 1.2077, 1.0965, 0.9622,\n",
       "          0.8745, 0.8116, 1.2560, 0.6697]],\n",
       "\n",
       "        [[1.2899, 1.0786, 1.2154, 0.8599, 1.4544, 0.9276, 0.9821, 1.3771,\n",
       "          1.0985, 1.0915, 1.0245, 1.0436, 1.2128, 1.0188, 0.9185, 0.8002,\n",
       "          0.7332, 0.6187, 0.9823, 0.7920]],\n",
       "\n",
       "        [[0.9396, 1.0833, 1.7402, 0.9264, 1.3507, 1.0361, 1.0854, 1.0791,\n",
       "          1.0686, 1.2008, 1.0723, 0.9008, 0.7258, 1.1320, 0.8909, 1.0877,\n",
       "          1.0223, 1.2808, 1.3244, 1.0989]],\n",
       "\n",
       "        [[0.7613, 1.2406, 1.7705, 1.0821, 1.0594, 0.9021, 0.7705, 1.0273,\n",
       "          1.1670, 0.9049, 1.1540, 0.9351, 1.3338, 1.2696, 1.0193, 0.8068,\n",
       "          0.6444, 1.2374, 1.5206, 1.0015]],\n",
       "\n",
       "        [[1.1415, 1.2133, 1.2418, 0.9583, 0.6831, 0.9023, 1.0056, 0.7636,\n",
       "          1.0151, 1.0878, 0.8434, 0.8716, 1.3527, 0.9913, 1.3551, 0.8423,\n",
       "          0.9321, 1.9130, 1.1013, 0.8503]],\n",
       "\n",
       "        [[0.5108, 0.8589, 1.1757, 1.0074, 1.0015, 0.9417, 1.1502, 1.1557,\n",
       "          1.1711, 1.7019, 0.9642, 1.2187, 1.5211, 1.4106, 1.3544, 0.8479,\n",
       "          0.7390, 1.1345, 1.2099, 1.4323]],\n",
       "\n",
       "        [[0.8716, 1.2880, 1.4495, 0.9228, 1.0663, 1.5262, 0.7859, 1.1788,\n",
       "          1.0829, 1.2446, 1.0411, 1.2227, 0.7245, 1.4530, 1.6028, 1.2317,\n",
       "          0.8664, 0.7387, 1.4922, 1.1794]],\n",
       "\n",
       "        [[0.7534, 1.1117, 1.8401, 0.8713, 1.2479, 0.7939, 1.0755, 1.2832,\n",
       "          1.2003, 0.7121, 1.0375, 0.9383, 0.7736, 1.1416, 1.2106, 0.7116,\n",
       "          0.9431, 0.6273, 1.3933, 1.0242]],\n",
       "\n",
       "        [[0.8418, 1.1486, 1.1714, 0.9571, 1.0721, 0.8653, 1.1112, 0.9523,\n",
       "          1.2115, 1.0002, 1.2147, 0.8547, 0.5334, 0.8490, 1.2467, 1.0366,\n",
       "          0.6477, 0.7694, 0.9509, 0.7756]],\n",
       "\n",
       "        [[1.0791, 1.2880, 1.4014, 1.2837, 1.0671, 0.6302, 0.6518, 0.6688,\n",
       "          1.3081, 0.8840, 0.8686, 0.6516, 0.9811, 0.9324, 1.3869, 0.8703,\n",
       "          0.7752, 1.2068, 1.0592, 0.7026]],\n",
       "\n",
       "        [[0.5716, 1.3638, 1.3087, 1.1214, 0.9045, 0.6209, 1.0506, 1.1076,\n",
       "          1.2817, 0.9210, 0.9177, 1.0313, 0.7048, 0.8289, 0.7856, 0.8975,\n",
       "          0.9355, 0.5416, 1.1423, 0.8630]],\n",
       "\n",
       "        [[0.6834, 1.0490, 0.9238, 0.8988, 0.8946, 1.3430, 0.9619, 0.7535,\n",
       "          1.1590, 1.3790, 0.8266, 0.7793, 0.7854, 1.0697, 1.3814, 1.1503,\n",
       "          0.8360, 1.0035, 1.0916, 0.9076]],\n",
       "\n",
       "        [[0.6055, 1.3579, 0.8563, 0.9769, 0.8326, 0.6279, 0.8112, 1.0309,\n",
       "          1.1399, 1.1658, 0.8786, 1.3702, 1.1242, 1.0807, 1.1883, 0.8655,\n",
       "          0.5672, 0.8868, 1.1178, 0.7467]],\n",
       "\n",
       "        [[0.6491, 1.2241, 1.3846, 1.1842, 1.1534, 0.9252, 1.2881, 0.9181,\n",
       "          0.9929, 1.2198, 0.9589, 0.8322, 1.0638, 1.0945, 1.0137, 0.9278,\n",
       "          1.2871, 1.2433, 1.4908, 0.8408]],\n",
       "\n",
       "        [[0.7331, 1.0669, 1.2922, 0.8810, 1.1202, 1.2869, 0.9295, 0.9993,\n",
       "          0.9819, 1.0417, 0.8424, 1.0931, 0.5072, 0.8523, 1.2802, 0.9529,\n",
       "          0.7111, 1.8137, 1.2625, 0.8044]],\n",
       "\n",
       "        [[0.5305, 1.2339, 1.2898, 1.3426, 1.1013, 0.6487, 1.0903, 0.6533,\n",
       "          1.1620, 0.8119, 0.9353, 1.3228, 0.9252, 0.9546, 1.4917, 0.8340,\n",
       "          0.7059, 1.7573, 1.6385, 0.7405]]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--cuda CUDA]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\75155\\AppData\\Roaming\\jupyter\\runtime\\kernel-558ca72b-ac81-431c-ba0d-2640b67bad83.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Training Parameter')\n",
    "parser.add_argument('--cuda', action='store', default=None, type=int)\n",
    "opt = parser.parse_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_iter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rollout.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main()第198行之后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.tensor([[2,50,100],\n",
    "                       [40,3,1000]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.view((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "512.667px",
    "left": "1009.33px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
