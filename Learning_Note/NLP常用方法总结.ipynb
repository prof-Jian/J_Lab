{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些工具的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vscode\n",
    "### 几个小技巧\n",
    "1. 查看函数或者类的定义\n",
    "`Ctrl`+`鼠标左键`点击函数名或者类名即可跳转到定义处，在函数名或者类名上按`F12`也可以实现同样功能\n",
    "\n",
    "\n",
    "2. 命名重构：\n",
    "在变量名上按`F2`即可实现重命名变量\n",
    "\n",
    "\n",
    "3. 方法重构:\n",
    "选中某一段代码，这个时候，代码的左侧会出现一个「灯泡图标」，点击这个图标，就可以把这段代码提取为一个单独的函数\n",
    "\n",
    "\n",
    "4. python断点调试:\n",
    "在行号的左边点击即可设置断点，在左边的调试界面可以查看变量的变化\n",
    "\n",
    "\n",
    "5. 函数在哪被调用了：\n",
    "选中`函数`（或者将光标放置在`函数`上），然后按住快捷键「Shift + F12」，就能看到`函数`在哪些地方被调用了，比较实用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恢复原来写过的代码\n",
    "场景：在某个窗口写了很多代码，又删除了很多单元格，想找回原来的代码。\n",
    "\n",
    "解决方法：直接在一个单元格中写入`history`就会展示出历史代码（前提是你运行过的，否则不会打印出来）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## win10的一些技巧\n",
    "- Sticky Note：Go to the Windows Ink Workspace  > Sticky Notes to create reminders for yourself. \n",
    "- Stay focused：Select and hold the window you want to stay open, then give your mouse (or finger) a little back-and-forth shake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch的一些知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在理解模型时一些有效的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_emb, emb_dim, hidden_dim, use_cuda):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_emb = num_emb\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_cuda = use_cuda\n",
    "        self.emb = nn.Embedding(num_emb, emb_dim) \n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True) \n",
    "        self.lin = nn.Linear(hidden_dim, num_emb)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        h0, c0 = self.init_hidden(x.size(0))\n",
    "        output, (h, c) = self.lstm(emb, (h0, c0)) \n",
    "        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim))) \n",
    "        return pred\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        c = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        if self.use_cuda:\n",
    "            h, c = h.cuda(), c.cuda()\n",
    "        return h, c\n",
    "    \n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "generator = Generator(num_emb = 5000,emb_dim = 128,hidden_dim = 64,use_cuda = 'Ture')\n",
    "generator = generator.cuda()\n",
    "x = torch.LongTensor([[2,50,100],\n",
    "                       [40,3,1000]]).cuda()\n",
    "pred = generator(x)\n",
    "params = list(generator.parameters())\n",
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.weight : torch.Size([5000, 128])\n",
      "lstm.weight_ih_l0 : torch.Size([256, 128])\n",
      "lstm.weight_hh_l0 : torch.Size([256, 64])\n",
      "lstm.bias_ih_l0 : torch.Size([256])\n",
      "lstm.bias_hh_l0 : torch.Size([256])\n",
      "lin.weight : torch.Size([5000, 64])\n",
      "lin.bias : torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in generator.named_parameters():\n",
    "    print(name,':',parameters.size())\n",
    "    # 需要值的话:parameters.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.5687, -8.4996, -8.4787,  ..., -8.4956, -8.5581, -8.5530],\n",
       "        [-8.5707, -8.5006, -8.4792,  ..., -8.4950, -8.5617, -8.5544],\n",
       "        [-8.5721, -8.5014, -8.4795,  ..., -8.4947, -8.5635, -8.5533],\n",
       "        [-8.5691, -8.4994, -8.4794,  ..., -8.4967, -8.5575, -8.5529],\n",
       "        [-8.5701, -8.5013, -8.4791,  ..., -8.4966, -8.5608, -8.5534],\n",
       "        [-8.5718, -8.5014, -8.4790,  ..., -8.4964, -8.5636, -8.5544]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 |Step: 0 |batch_x: [6. 7. 5. 3. 4.] |batch_y: [5. 4. 6. 8. 7.]\n",
      "Epoch: 0 |Step: 1 |batch_x: [10.  1.  9.  2.  8.] |batch_y: [ 1. 10.  2.  9.  3.]\n",
      "Epoch: 1 |Step: 0 |batch_x: [ 8.  7.  1.  2. 10.] |batch_y: [ 3.  4. 10.  9.  1.]\n",
      "Epoch: 1 |Step: 1 |batch_x: [9. 4. 6. 3. 5.] |batch_y: [2. 7. 5. 8. 6.]\n",
      "Epoch: 2 |Step: 0 |batch_x: [ 3.  5. 10.  6.  9.] |batch_y: [8. 6. 1. 5. 2.]\n",
      "Epoch: 2 |Step: 1 |batch_x: [7. 1. 2. 8. 4.] |batch_y: [ 4. 10.  9.  3.  7.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(1,10,10) #linspace(star,end,step):从1到10，10个step走完\n",
    "y = torch.linspace(10,1,10)\n",
    "\n",
    "import torch.utils.data as Data\n",
    "torch_dataset = Data.TensorDataset(x,y)\n",
    "loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 4)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for step,(batch_x,batch_y) in enumerate(loader):\n",
    "        print('Epoch:',epoch,'|Step:',step,'|batch_x:',batch_x.numpy(),\n",
    "             '|batch_y:',batch_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=torch.int32) tensor([10,  9,  8,  7,  6,  5,  4,  3,  2,  1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "k = []\n",
    "j = []\n",
    "for i in range(10):\n",
    "    k.append(i+1)\n",
    "    j.append(10-i)\n",
    "a = np.array(k)\n",
    "b = np.array(j)\n",
    "a = torch.from_numpy(a)\n",
    "b = torch.from_numpy(b)\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 |Step: 0 |batch_x: [1 2 8 3 6] |batch_y: [10  9  3  8  5]\n",
      "Epoch: 0 |Step: 1 |batch_x: [ 5  7  9  4 10] |batch_y: [6 4 2 7 1]\n",
      "Epoch: 1 |Step: 0 |batch_x: [3 1 6 5 9] |batch_y: [ 8 10  5  6  2]\n",
      "Epoch: 1 |Step: 1 |batch_x: [ 7  2  4  8 10] |batch_y: [4 9 7 3 1]\n",
      "Epoch: 2 |Step: 0 |batch_x: [8 3 1 2 4] |batch_y: [ 3  8 10  9  7]\n",
      "Epoch: 2 |Step: 1 |batch_x: [ 6  5  9 10  7] |batch_y: [5 6 2 1 4]\n"
     ]
    }
   ],
   "source": [
    "k = []\n",
    "j = []\n",
    "for i in range(10):\n",
    "    k.append(i+1)\n",
    "    j.append(10-i)\n",
    "a = np.array(k)\n",
    "b = np.array(j)\n",
    "\n",
    "class My_Dataset(Data.TensorDataset): # 这里Data.Dataset也可运行\n",
    "    def __init__(self,x,y):\n",
    "        self.data_x = torch.from_numpy(x)\n",
    "        self.data_y = torch.from_numpy(y)\n",
    "        self.len = len(x)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.data_x[index],self.data_y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "my_dataset = My_Dataset(a,b)\n",
    "my_loader = Data.DataLoader(\n",
    "    dataset = my_dataset,\n",
    "    batch_size = 5,\n",
    "    shuffle = True, # num_workers = 2会出错，放在main()函数里就不会出错；\n",
    "    )\n",
    "\n",
    "for epoch in range(3):\n",
    "    for step,(batch_x,batch_y) in enumerate(my_loader):\n",
    "        print('Epoch:',epoch,'|Step:',step,'|batch_x:',batch_x.numpy(),\n",
    "             '|batch_y:',batch_y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个input_size = 1 ， hidden_size = 20 ，num_layers = 1\n",
    "rnn = nn.LSTM(10,20,1,batch_first = True) # batch_first不影响h和c\n",
    "input_1 = torch.randn(3, 5, 10)\n",
    "h0 = torch.randn(1, 3, 20)\n",
    "c0 = torch.randn(1, 3, 20)\n",
    "output, (hn, cn) = rnn(input_1, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 20])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.multinomial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor\n",
    "\n",
    "```python\n",
    ">>> weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights\n",
    ">>> torch.multinomial(weights, 4) #可以试试重复运行这条命令，发现只会有2种结果：[1 2 0 0]以及[2 1 0 0]，以[1 2 0 0]这种情况居多。\n",
    " 1\n",
    " 2\n",
    " 0\n",
    " 0\n",
    "[torch.LongTensor of size 4]\n",
    " \n",
    ">>> torch.multinomial(weights, 4, replacement=True)\n",
    " 1\n",
    " 2\n",
    " 1\n",
    " 2\n",
    "[torch.LongTensor of size 4]\n",
    "```\n",
    "- input张量可以看成一个权重张量，每一个元素代表其在该行中的权重。如果有元素为0，那么在其他不为0的元素被取干净之前，这个元素是不会被取到的。\n",
    "- n_samples是每一行的取值次数，该值不能大于每一样的元素数，否则会报错。\n",
    "- replacement指的是取样时是否是有放回的取样，True是有放回，False无放回。\n",
    "- 输入二维张量，则返回的也会成为一个二维张量，行数为输入的行数，列数为n_samples，即每一行都取了n_samples次，取法和一维张量相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.Softmax  & torch.nn.LogSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax函数定义：\n",
    "$ f(x_i)=\\frac{e^{(x_{i}-shift)}}{\\sum_{j}e^{(x_{j}-shift)}} $,其中 $ shift = max(x_i)$\n",
    "\n",
    "**以下是官方文档，但是貌似实际按上面的计算**\n",
    "- 函数Softmax(X) \n",
    "\n",
    "其中$ X = (x_1,x_2,...) $\n",
    "$$ Softmax(X) = (\\frac{e^{x_1}}{\\sum e^{x_i}},\\frac{e^{x_2}}{\\sum e^{x_i}},...) $$\n",
    "\n",
    "\n",
    "- log_softmax(X)\n",
    "\n",
    "其中$ X = (x_1,x_2,...) $\n",
    "$$ log-Softmax(X) = (log\\frac{e^{x_1}}{\\sum e^{x_i}},log\\frac{e^{x_2}}{\\sum e^{x_i}},...) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input= tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "output(dim=0)=\n",
      " tensor([[3.2932e-04, 3.2932e-04, 3.2932e-04, 3.2932e-04],\n",
      "        [1.7980e-02, 1.7980e-02, 1.7980e-02, 1.7980e-02],\n",
      "        [9.8169e-01, 9.8169e-01, 9.8169e-01, 9.8169e-01]])\n",
      "output(dim=1)=\n",
      " tensor([[0.0321, 0.0871, 0.2369, 0.6439],\n",
      "        [0.0321, 0.0871, 0.2369, 0.6439],\n",
      "        [0.0321, 0.0871, 0.2369, 0.6439]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output(dim=default)=\n",
      " tensor([[0.0321, 0.0871, 0.2369, 0.6439],\n",
      "        [0.0321, 0.0871, 0.2369, 0.6439],\n",
      "        [0.0321, 0.0871, 0.2369, 0.6439]])\n"
     ]
    }
   ],
   "source": [
    "input1 = torch.Tensor(np.arange(12)).view(3,4)\n",
    "print('input=',input1)\n",
    "m = nn.Softmax(dim = 0)\n",
    "n = nn.Softmax(dim = 1)\n",
    "k = nn.Softmax()\n",
    "print('output(dim=0)=\\n',m(input1))\n",
    "print('output(dim=1)=\\n',n(input1))\n",
    "print('output(dim=default)=\\n',k(input1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08714431874203257\n"
     ]
    }
   ],
   "source": [
    "a = math.exp(-2)/(math.exp(-3)+math.exp(-2)+math.exp(-1)+1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output(dim=default)=\n",
      " tensor([[-3.4402, -2.4402, -1.4402, -0.4402],\n",
      "        [-3.4402, -2.4402, -1.4402, -0.4402],\n",
      "        [-3.4402, -2.4402, -1.4402, -0.4402]])\n"
     ]
    }
   ],
   "source": [
    "t = nn.LogSoftmax()\n",
    "print('output(dim=default)=\\n',t(input1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.4401896985611953\n"
     ]
    }
   ],
   "source": [
    "print(math.log(0.08714431874203257))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.CrossEntropyLoss()和nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLLLoss(negative log likelihood loss)\n",
    "\n",
    "\n",
    "- CrossEntropyLoss\n",
    "\n",
    "$ Y = (y_1,y_2,...) $是target，而且是one-hot标签，$ P = (P_1,P_2,...) $是经过Softmax层输出的pred\n",
    "\n",
    "$$ E = -\\sum_{1}^{n}y_{i} * log(P_{i})$$ \n",
    "\n",
    "- **CrossEntropyLoss() = log_softmax() + NLLLoss()**\n",
    "> This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
    "\n",
    "$$CrossEntropyLoss(X,class) = - log\\frac{e^{x_{class}}}{\\sum e^{x_i}}$$\n",
    "\n",
    "> input:(N,C),N = Batch_size,C = num_classes #其中每个datapoint也就是每一横排，就是上面的X\n",
    "\n",
    "> Target:(N)  #一定要注意这不是(N,1)\n",
    "\n",
    "> Output:scalar # 只和相应的$ x_{class}$大小相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.LongTensor([1,0,0,1])\n",
    "pred = torch.Tensor([-988,-0.01,-0.0005,-1080,-0.009,-3880,-180,-0.001]).view(4,2)\n",
    "# 上面改成pred = torch.Tensor([-98,-0.01,-0.0005,-100,-0.009,-300,-10,-0.001]).view(4,2)结果一样\n",
    "dis_criterion = nn.NLLLoss(reduction='sum')\n",
    "a = dis_criterion(pred,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0205)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.8800e+02, -1.0000e-02],\n",
       "        [-5.0000e-04, -1.0800e+03],\n",
       "        [-9.0000e-03, -3.8800e+03],\n",
       "        [-1.8000e+02, -1.0000e-03]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input1 = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5) # 应为每一行有5个预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3260,  1.0335,  1.0452,  1.4386, -0.9655],\n",
       "        [-0.0737,  0.8103,  0.4304,  0.1323, -0.3928],\n",
       "        [ 0.1206, -2.6637, -0.0357,  1.7111, -0.9470]], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1972, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = loss(input1,target)\n",
    "aaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.multinomial是否可以back_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**凡是有embedding层的，都不能反向传导？？？？**\n",
    "并不是这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_emb, emb_dim, hidden_dim, use_cuda):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_emb = num_emb\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_cuda = use_cuda\n",
    "        self.emb = nn.Embedding(num_emb, emb_dim) \n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True) \n",
    "        self.lin = nn.Linear(hidden_dim, num_emb)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        h0, c0 = self.init_hidden(x.size(0))\n",
    "        output, (h, c) = self.lstm(emb, (h0, c0)) \n",
    "        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim))) \n",
    "        return pred\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        c = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        if self.use_cuda:\n",
    "            h, c = h.cuda(), c.cuda()\n",
    "        return h, c\n",
    "    \n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "generator = Generator(num_emb = 1000,emb_dim = 128,hidden_dim = 64,use_cuda = 'Ture')\n",
    "generator = generator.cuda()\n",
    "x = torch.LongTensor([[2,50,100],\n",
    "                       [40,3,900]]).cuda()\n",
    "target = torch.LongTensor([[600],[233],[111],[500],[700],[132]]).cuda().contiguous().view(-1)\n",
    "# 以上这个target不加contiguous().view(-1)会出错\n",
    "loss = nn.NLLLoss()\n",
    "optimizer = optim.Adam(generator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.9089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.9039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.8098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.7988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.7866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.7732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(6.7584, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 重复这个cell可以让最后的结果一直减少\n",
    "for i in range(20):\n",
    "    pred = generator(x)\n",
    "    loss_scale = loss(pred,target)\n",
    "    optimizer.zero_grad()\n",
    "    loss_scale.backward()\n",
    "    optimizer.step()\n",
    "    print(loss_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入pred_index = pred.multinomial(1)可以有反向传播吗？没有反向传播会怎么报错？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_emb, emb_dim, hidden_dim, use_cuda):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_emb = num_emb\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_cuda = use_cuda\n",
    "        self.emb = nn.Embedding(num_emb, emb_dim) \n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True) \n",
    "        self.lin = nn.Linear(hidden_dim, num_emb)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        h0, c0 = self.init_hidden(x.size(0))\n",
    "        output, (h, c) = self.lstm(emb, (h0, c0)) \n",
    "        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim)))\n",
    "        pred = - pred\n",
    "        pred_index = pred.multinomial(1)\n",
    "        return pred_index\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        c = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        if self.use_cuda:\n",
    "            h, c = h.cuda(), c.cuda()\n",
    "        return h, c\n",
    "    \n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "generator = Generator(num_emb = 1000,emb_dim = 64,hidden_dim = 64,use_cuda = 'Ture')\n",
    "generator = generator.cuda()\n",
    "\n",
    "x = torch.LongTensor([[2,50,100],\n",
    "                       [40,3,900]]).cuda()\n",
    "target = torch.LongTensor([[600],[233],[111],[500],[700],[132]]).cuda()\n",
    "optimizer = optim.Adam(generator.parameters())\n",
    "Loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(263116., device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(122201.1641, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(173140.5000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(184143.8281, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(160360.6719, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(84713.8359, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(121222., device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(89633.1641, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(190037.5000, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(247259.5000, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    pred = generator(x)\n",
    "    pred=Variable(pred.float(),requires_grad=True)\n",
    "    loss_scale = Loss(pred,target.float())\n",
    "    optimizer.zero_grad()\n",
    "    loss_scale.backward()\n",
    "    optimizer.step()\n",
    "    print(loss_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上不收敛是因为multinomial()导致的吗？怎么验证我的观点？？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.embding()出错\n",
    "\n",
    "> RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got CPUType instead (while checking arguments for embedding)\n",
    "\n",
    "结果是因为，`nn.embding()`的输入必须是LongTensor型，所以在用numpy处理时，数组`dtype = np.int64`，注意这个不是`int64`，哈哈哈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor.contiguous().view( , ):**\n",
    "\n",
    "有些tensor并不是占用一整块内存，而是由不同的数据块组成，而tensor的view()操作依赖于内存是整块的，这时只需要执行contiguous()这个函数，把tensor变成在内存中连续分布的形式。\n",
    "判断是否contiguous用torch.Tensor.is_contiguous()函数。\n",
    "```python\n",
    "x = torch.ones(10, 10)\n",
    "x.is_contiguous()  # True\n",
    "x.transpose(0, 1).is_contiguous()  # False\n",
    "x.transpose(0, 1).contiguous().is_contiguous()  # True\n",
    "```\n",
    "\n",
    "*目前只看过lstm函数的输出如果要view()，则先要contiguous.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 清洗数据和文件读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存和读取list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import open\n",
    "def text_save(content,filename,mode='a'):\n",
    "    # Try to save a list variable in txt file.\n",
    "    file = open(filename,mode)\n",
    "    for i in range(len(content)):\n",
    "        file.write(str(content[i])+'\\n')\n",
    "    file.close()\n",
    "\n",
    "def text_read(filename):\n",
    "    # Try to read a txt file and return a list.Return [] if there was a mistake.\n",
    "    try:\n",
    "        file = open(filename,'r')\n",
    "    except IOError:\n",
    "        error = []\n",
    "        return error\n",
    "    content = file.readlines()\n",
    "\n",
    "    for i in range(len(content)):\n",
    "        content[i] = content[i][:len(content[i])-1]\n",
    "\n",
    "    file.close()\n",
    "    return content\n",
    "```\n",
    "\n",
    "实际运行：\n",
    "```python\n",
    "test_text = ['just','for','test']\n",
    "text_save(test_text,'1.txt')\n",
    "```\n",
    "得到1.txt文档：\n",
    "> just\n",
    "\n",
    "> for\n",
    "\n",
    "> test\n",
    "\n",
    "读取1.txt：\n",
    "```python\n",
    "test_content = text_read('1.txt')\n",
    "print(test_content)\n",
    "```\n",
    "得到：\n",
    "`['just', 'for', 'test']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取txt文件\n",
    "```python\n",
    "from os.path import join\n",
    "from codecs import open\n",
    "raw_nomal_dataset_address = '.\\\\training dataset v4\\\\01-一般项目\\\\'\n",
    "with open(join(raw_nomal_dataset_address,'一般项目-'+str(i+1)+'.txtoriginal.txt'), 'r', encoding='utf-8') as f:\n",
    "    @#$%^&*#$%^&*#$%^&*\n",
    "```\n",
    "**这里备注一下：**\n",
    "\n",
    "在《保存和读取list》这个经验里不用写`encoding='utf-8'`，否则会报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## startswith()函数\n",
    "```python\n",
    ">>> str = \"this is string example....wow!!!\"\n",
    "print (str.startswith( 'this' ))   # 字符串是否以 this 开头\n",
    "print (str.startswith( 'string', 8 ))  # 从第八个字符开始的字符串是否以 string 开头\n",
    "print (str.startswith( 'this', 2, 4 )) # 从第2个字符开始到第四个字符结束的字符串是否以 this 开头\n",
    ">>> True\n",
    "True\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python删除和替换字符串中指定字符\n",
    "\n",
    "###  str.maketrans(x[, y[, z]])和str.translate()的组合\n",
    "`static str.maketrans(x[, y[, z]])`\n",
    "This static method returns a translation table usable for `str.translate()`.\n",
    "\n",
    "- If there is only one argument, \n",
    "it must be a dictionary mapping Unicode ordinals (integers) or characters (strings of length 1) to Unicode ordinals, \n",
    "strings (of arbitrary lengths) or None. Character keys will then be converted to ordinals.\n",
    "\n",
    "- If there are two arguments, \n",
    "they must be strings of equal length, \n",
    "and in the resulting dictionary, \n",
    "each character in x will be mapped to the character at the same position in y. \n",
    "- If there is a third argument, it must be a string, whose characters will be mapped to None in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello how are you'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "i = 'Hello, how are you!'\n",
    "i.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heo word i am i'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 'hello world i am li'\n",
    "i.translate(str.maketrans('','','l'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th3s 3s str3ng 2x1mpl2....w4w!!!\n"
     ]
    }
   ],
   "source": [
    "intab = \"aeiou\"\n",
    "outtab = \"12345\"\n",
    "trantab = str.maketrans(intab, outtab)\n",
    " \n",
    "str = \"this is string example....wow!!!\";\n",
    "print(str.translate(trantab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khis is sKring example....PoP!!!\n"
     ]
    }
   ],
   "source": [
    "a = str.maketrans({'t': 'K', 'w': 'P'}) # 这个词典的key必须长度为1，而value的长度没有要求\n",
    "stra = \"this is string example....wow!!!\";\n",
    "print(stra.translate(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th3s 3s str3ng 21pl2....w4w!!!\n"
     ]
    }
   ],
   "source": [
    "# 删去 xm\n",
    "intab = \"aeiou\"\n",
    "outtab = \"12345\"\n",
    "trantab = str.maketrans(intab, outtab, 'xm')\n",
    " \n",
    "str = \"this is string example....wow!!!\";\n",
    "print(str.translate(trantab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### str.replace(old, new[, max])\n",
    "- old -- 将被替换的子字符串；\n",
    "- new -- 新字符串，用于替换old子字符串；\n",
    "- max -- 可选字符串, 替换不超过 max 次；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thwas was string example....wow!!! thwas was really string\n",
      "thwas was string example....wow!!! thwas is really string\n",
      "th  string example....wow!!! th  really string\n"
     ]
    }
   ],
   "source": [
    "str = \"this is string example....wow!!! this is really string\";\n",
    "print(str.replace(\"is\", \"was\"))\n",
    "print(str.replace(\"is\", \"was\", 3))\n",
    "print(str.replace(\"is\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## os.walk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`os.walk(top[, topdown=True[, onerror=None[, followlinks=False]]])`\n",
    "\n",
    "参数:\n",
    "- top -- 是你所要遍历的目录的地址, 返回的是一个三元组(root,dirs,files)。\n",
    "- root 所指的是当前正在遍历的这个文件夹的本身的地址\n",
    "- dirs 是一个 list ，内容是该文件夹中所有的目录的名字(不包括子目录)\n",
    "- files 同样是 list , 内容是该文件夹中所有的文件(不包括子目录)\n",
    "- topdown --可选，为 True，则优先遍历 top 目录，否则优先遍历 top 的子目录(默认为开启)。如果 topdown 参数为 True，walk 会遍历top文件夹，与top 文件夹中每一个子目录。\n",
    "\n",
    "- onerror -- 可选，需要一个 callable 对象，当 walk 需要异常时，会调用。\n",
    "\n",
    "- followlinks -- 可选，如果为 True，则会遍历目录下的快捷方式(linux 下是软连接 symbolic link )实际所指的目录(默认关闭)，如果为 False，则优先遍历 top 的子目录。\n",
    "\n",
    "示例：\n",
    "```python\n",
    "import os\n",
    "origin_path = 'E:\\\\github lab\\\\data\\\\ren_task\\\\training dataset v4\\\\02-病史特点\\\\'\n",
    "tag_list = []\n",
    "for root,dirs,files in os.walk(origin_path):\n",
    "    for file in files:\n",
    "        label_filepath = os.path.join(root,file)\n",
    "        if 'original' not in label_filepath:\n",
    "            for line in open(label_filepath, 'r', encoding='utf-8'):\n",
    "                *term,b,e,tag = line.strip().split()\n",
    "                tag_list.extend(term)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些常见的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## squeeze()和unsqueeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> x = torch.zeros(2, 1, 2, 1, 2)\n",
    ">>> x.size()\n",
    "torch.Size([2, 1, 2, 1, 2])\n",
    ">>> y = torch.squeeze(x)\n",
    ">>> y.size()\n",
    "torch.Size([2, 2, 2])\n",
    ">>> y = torch.squeeze(x, 0)\n",
    ">>> y.size()\n",
    "torch.Size([2, 1, 2, 1, 2])\n",
    ">>> y = torch.squeeze(x, 1)\n",
    ">>> y.size()\n",
    "torch.Size([2, 2, 1, 2])\n",
    "```\n",
    "------\n",
    "```python\n",
    ">>> x = np.array([[[0], [1], [2]]])\n",
    ">>> x.shape\n",
    "(1, 3, 1)\n",
    ">>> np.squeeze(x).shape\n",
    "(3,)\n",
    ">>> np.squeeze(x, axis=0).shape\n",
    "(3, 1)\n",
    ">>> np.squeeze(x, axis=1).shape\n",
    "Traceback (most recent call last):\n",
    "...\n",
    "ValueError: cannot select an axis to squeeze out which has size not equal to one\n",
    ">>> np.squeeze(x, axis=2).shape\n",
    "(1, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @classmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_test2(object):\n",
    "    day=0\n",
    "    month=0\n",
    "    year=0\n",
    "    def __init__(self,year=0,month=0,day=0):\n",
    "        self.day=day\n",
    "        self.month=month\n",
    "        self.year=year\n",
    "\n",
    "    @classmethod\n",
    "    def get_date(cls,data_as_string):\n",
    "        #这里第一个参数是cls， 表示调用当前的类名\n",
    "        year,month,day = map(int,data_as_string.split('-'))\n",
    "        return cls(year,month,day)\n",
    "\n",
    "    def out_date(self):\n",
    "        print (\"year :\")\n",
    "        print (self.year)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Data_test2.get_date('2019-7-13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year :\n",
      "2019\n"
     ]
    }
   ],
   "source": [
    "t.out_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同type的一些操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dateFrame格式的一些操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取时添加添加header\n",
    "```python\n",
    "x = pd.read_csv(ls,header = None,names = [\"num\",\"term\",\"context\"]) # 给列表加上‘num’‘term’‘context'几个表头\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataFrame删除一行或一列：drop函数（相应的行标和列表也会跟着删除，特别的，删除行，行标就不再是连续的了）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用法：DataFrame.drop(labels=None,axis=0, index=None, columns=None, inplace=False)\n",
    "\n",
    "------\n",
    "参数说明：\n",
    "- labels 就是要删除的行列的名字，用列表给定\n",
    "- axis 默认为0，指删除行，因此删除columns时要指定axis=1；\n",
    "- index 直接指定要删除的行\n",
    "- columns 直接指定要删除的列\n",
    "- inplace=False，默认该删除操作不改变原数据，而是返回一个执行删除操作后的新dataframe；\n",
    "- inplace=True，则会直接在原数据上进行删除操作，删除后无法返回。\n",
    "\n",
    "因此，删除行列有两种方式：\n",
    "1. labels=None,axis=0 的组合\n",
    "2. index或columns直接指定要删除的行或列\n",
    "\n",
    "例子：\n",
    "```python\n",
    ">>>df = pd.DataFrame(np.arange(12).reshape(3,4), columns=['A', 'B', 'C', 'D'])\n",
    "\n",
    ">>>df\n",
    "\n",
    "   A   B   C   D\n",
    "\n",
    "0  0   1   2   3\n",
    "\n",
    "1  4   5   6   7\n",
    "\n",
    "2  8   9  10  11\n",
    "\n",
    "#Drop columns,两种方法等价\n",
    "\n",
    ">>>df.drop(['B', 'C'], axis=1)\n",
    "\n",
    "   A   D\n",
    "\n",
    "0  0   3\n",
    "\n",
    "1  4   7\n",
    "\n",
    "2  8  11\n",
    "\n",
    ">>>df.drop(columns=['B', 'C'])\n",
    "\n",
    "   A   D\n",
    "\n",
    "0  0   3\n",
    "\n",
    "1  4   7\n",
    "\n",
    "2  8  11\n",
    "\n",
    "# 第一种方法下删除column一定要指定axis=1,否则会报错\n",
    ">>> df.drop(['B', 'C'])\n",
    "\n",
    "ValueError: labels ['B' 'C'] not contained in axis\n",
    "\n",
    "#Drop rows\n",
    ">>>df.drop([0, 1])\n",
    "\n",
    "   A  B   C   D\n",
    "\n",
    "2  8  9  10  11\n",
    "\n",
    ">>> df.drop(index=[0, 1])\n",
    "\n",
    "   A  B   C   D\n",
    "   \n",
    "2  8  9  10  11\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遍历DataFrame的行：DataFrame.iterrows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>>import pandas as pd\n",
    "inp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\n",
    "df = pd.DataFrame(inp)\n",
    "print(df)\n",
    ">>>\n",
    "   c1   c2\n",
    "0  10  100\n",
    "1  11  110\n",
    "2  12  120\n",
    ">>>for index, row in df.iterrows():\n",
    "    print row[\"c1\"], row[\"c2\"]\n",
    ">>>10 100\n",
    "11 110\n",
    "12 120\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dateFrame.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ls = 'E:\\\\reading_books\\\\PyTorchNLPBook-master\\\\data\\\\surnames\\\\surnames_with_splits.csv'\n",
    "x = pd.read_csv(ls,header = 0)\n",
    "clip = x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>nationality</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Totah</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Abboud</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Fakhoury</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Srour</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sayegh</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname nationality  split\n",
       "0     Totah      Arabic  train\n",
       "1    Abboud      Arabic  train\n",
       "2  Fakhoury      Arabic  train\n",
       "3     Srour      Arabic  train\n",
       "4    Sayegh      Arabic  train"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_1 = clip.nationality\n",
    "d_1 = clip['nationality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    Arabic\n",
       " 1    Arabic\n",
       " 2    Arabic\n",
       " 3    Arabic\n",
       " 4    Arabic\n",
       " Name: nationality, dtype: object, 0    Arabic\n",
       " 1    Arabic\n",
       " 2    Arabic\n",
       " 3    Arabic\n",
       " 4    Arabic\n",
       " Name: nationality, dtype: object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_1,d_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality = x.nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English       2972\n",
       "Russian       2373\n",
       "Arabic        1603\n",
       "Japanese       775\n",
       "Italian        600\n",
       "German         576\n",
       "Czech          414\n",
       "Spanish        258\n",
       "Dutch          236\n",
       "French         229\n",
       "Chinese        220\n",
       "Irish          183\n",
       "Greek          156\n",
       "Polish         120\n",
       "Korean          77\n",
       "Scottish        75\n",
       "Vietnamese      58\n",
       "Portuguese      55\n",
       "Name: nationality, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English': 2972,\n",
       " 'Russian': 2373,\n",
       " 'Arabic': 1603,\n",
       " 'Japanese': 775,\n",
       " 'Italian': 600,\n",
       " 'German': 576,\n",
       " 'Czech': 414,\n",
       " 'Spanish': 258,\n",
       " 'Dutch': 236,\n",
       " 'French': 229,\n",
       " 'Chinese': 220,\n",
       " 'Irish': 183,\n",
       " 'Greek': 156,\n",
       " 'Polish': 120,\n",
       " 'Korean': 77,\n",
       " 'Scottish': 75,\n",
       " 'Vietnamese': 58,\n",
       " 'Portuguese': 55}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationality.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator类的一些操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ls = 'E:\\\\reading_books\\\\PyTorchNLPBook-master\\\\data\\\\surnames\\\\surnames_with_splits.csv'\n",
    "x = pd.read_csv(ls,header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>nationality</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Totah</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abboud</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fakhoury</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Srour</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sayegh</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cham</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Haik</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kattan</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Khouri</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Antoun</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wasem</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Srour</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Seif</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Guirguis</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sarkis</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Said</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Malouf</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bishara</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sarkis</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Totah</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ganim</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Baz</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Assaf</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nader</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Isa</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Awad</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Deeb</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Kanaan</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Quraishi</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Atiyeh</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10932</th>\n",
       "      <td>Trinh</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10933</th>\n",
       "      <td>Phi</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10934</th>\n",
       "      <td>Thai</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10935</th>\n",
       "      <td>Vuong</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10936</th>\n",
       "      <td>Trang</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>Vo</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10938</th>\n",
       "      <td>Huynh</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10939</th>\n",
       "      <td>Trieu</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10940</th>\n",
       "      <td>an</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10941</th>\n",
       "      <td>Duong</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10942</th>\n",
       "      <td>Luc</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10943</th>\n",
       "      <td>Truong</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10944</th>\n",
       "      <td>Giang</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10945</th>\n",
       "      <td>Vinh</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10946</th>\n",
       "      <td>Luong</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10947</th>\n",
       "      <td>Dam</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10948</th>\n",
       "      <td>Luu</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10949</th>\n",
       "      <td>Ta</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10950</th>\n",
       "      <td>Vuu</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10951</th>\n",
       "      <td>To</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>Mai</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10953</th>\n",
       "      <td>Lieu</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>Sai</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>Chu</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10956</th>\n",
       "      <td>Dao</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10957</th>\n",
       "      <td>Chu</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10958</th>\n",
       "      <td>Pham</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10959</th>\n",
       "      <td>Chau</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10960</th>\n",
       "      <td>Mach</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10961</th>\n",
       "      <td>Tieu</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7680 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        surname nationality  split\n",
       "0         Totah      Arabic  train\n",
       "1        Abboud      Arabic  train\n",
       "2      Fakhoury      Arabic  train\n",
       "3         Srour      Arabic  train\n",
       "4        Sayegh      Arabic  train\n",
       "...         ...         ...    ...\n",
       "10957       Chu  Vietnamese  train\n",
       "10958      Pham  Vietnamese  train\n",
       "10959      Chau  Vietnamese  train\n",
       "10960      Mach  Vietnamese  train\n",
       "10961      Tieu  Vietnamese  train\n",
       "\n",
       "[7680 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[x.split=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "a = x[x.split=='train']  \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "        \n",
    "batch_generator = generate_batches(a, batch_size=64, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "882",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 882",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-467d4407f7e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatch_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-f13be86aa753>\u001b[0m in \u001b[0;36mgenerate_batches\u001b[1;34m(dataset, batch_size, shuffle, drop_last, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m                             shuffle=shuffle, drop_last=drop_last)\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mout_data_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2978\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 882"
     ]
    }
   ],
   "source": [
    "batch_generator.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4140",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 4140",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ab8bbc602317>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_dict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e712325aa6bd>\u001b[0m in \u001b[0;36mgenerate_batches\u001b[1;34m(dataset, batch_size, shuffle, drop_last, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m                             shuffle=shuffle, drop_last=drop_last)\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mout_data_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2978\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 4140"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "b = []\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    print(batch_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "b_size = (5,)\n",
    "b = np.zeros(b_size, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchTensor类的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 维度转换\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a=np.array([[[1,2,3],[4,5,6]]])\n",
    "\n",
    "unpermuted=torch.tensor(a)\n",
    "print(unpermuted.size())  #  ——>  torch.Size([1, 2, 3])\n",
    "\n",
    "permuted=unpermuted.permute(2,0,1)\n",
    "print(permuted.size())     #  ——>  torch.Size([3, 1, 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.stack()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a:\n",
      " tensor([[ 1,  2,  3],\n",
      "        [11, 22, 33]], dtype=torch.int32) \n",
      "b:\n",
      " tensor([[ 4,  5,  6],\n",
      "        [44, 55, 66]], dtype=torch.int32) \n",
      "c:\n",
      " tensor([[[ 1,  2,  3],\n",
      "         [11, 22, 33]],\n",
      "\n",
      "        [[ 4,  5,  6],\n",
      "         [44, 55, 66]]], dtype=torch.int32) \n",
      "d:\n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[11, 22, 33],\n",
      "         [44, 55, 66]]], dtype=torch.int32) \n",
      "e:\n",
      " tensor([[[ 1,  4],\n",
      "         [ 2,  5],\n",
      "         [ 3,  6]],\n",
      "\n",
      "        [[11, 44],\n",
      "         [22, 55],\n",
      "         [33, 66]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.IntTensor([[1,2,3],[11,22,33]])\n",
    "b = torch.IntTensor([[4,5,6],[44,55,66]])\n",
    "\n",
    "c = torch.stack([a,b],dim = 0)  # c = [ a, b]\n",
    "d = torch.stack([a,b],dim = 1)  # d = [ [a[0] , b[0] ] , [a[1], b[1] ] ]\n",
    "e = torch.stack([a,b],dim = 2)  # e = [[[ a[0][0], b[0][0]],[a[0][1],b[0][1]],[a[0][2],b[0][2]]] ,\n",
    "#                         [[ a[1][0], b[1][0]],[a[1][1],b[0][1]],[a[1][2],b[1][2]]]]\n",
    "print('\\na:\\n',a,'\\nb:\\n',b,'\\nc:\\n',c,'\\nd:\\n',d,'\\ne:\\n',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor.chunk()函数\n",
    "> torch.chunk(input, chunks, dim=0) → List of Tensors\n",
    "\n",
    "Splits a tensor into a specific number of chunks.\n",
    "Last chunk will be smaller if the tensor size along the given dimension `dim` is not divisible by `chunks`.\n",
    "\n",
    "> Parameters\n",
    "- input(Tensor):the tensor to split\n",
    "- chunks(int):number of chunks to return\n",
    "- dim(int):dimension along which to split the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [4],\n",
       "         [8]], dtype=torch.int32), tensor([[1],\n",
       "         [5],\n",
       "         [9]], dtype=torch.int32), tensor([[ 2],\n",
       "         [ 6],\n",
       "         [10]], dtype=torch.int32), tensor([[ 3],\n",
       "         [ 7],\n",
       "         [11]], dtype=torch.int32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis = a.chunk(4,dim=1)\n",
    "lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python的列表（list）、字符串"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向输出和拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcba [5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "string = 'abcd'\n",
    "my_list = [1,2,3,4,5]\n",
    "print(string[::-1],my_list[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is awesome!\n"
     ]
    }
   ],
   "source": [
    "word_list = ['awesome','is','this']\n",
    "print(' '.join(word_list[::-1])+'!') # 注意前面那个空格，改为其他任何东都可以！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列表推导式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 14, 30]\n"
     ]
    }
   ],
   "source": [
    "def my_func(x):\n",
    "    return x**2 + 5\n",
    "my_list = [1,2,3,4,5]\n",
    "'''\n",
    "new_list = []\n",
    "for x in my_list:\n",
    "    if x % 2 != 0:\n",
    "        new_list.append(my_func(x))\n",
    "print(new_list) # [6,14,30]\n",
    "'''\n",
    "print([my_func(x) for x in my_list if x % 2 != 0]) #这个等价很帅！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 14, 30]\n"
     ]
    }
   ],
   "source": [
    "print([x ** 2 +5 for x in my_list if x % 2 != 0]) #这个更加直接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**综上：**\n",
    "\n",
    "```python\n",
    "expression for item in list if conditional\n",
    "```\n",
    "等价于\n",
    "```python\n",
    "for item in list:\n",
    "    if conditional:\n",
    "        expression\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy对象切片的差别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.shape= (6,) \t c.shape (6, 1)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([i for i in range(12)])\n",
    "a = a.reshape((6,2))\n",
    "b = a[:,1]\n",
    "c = a[:,0:1]\n",
    "print('b.shape=',b.shape,'\\t','c.shape',c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习中的AUC\n",
    "\n",
    "𝐴𝑈𝐶 的衡量标准对二分类非常易用（特别是本省的数据是Unbalanced的时候）[https://cloud.tencent.com/developer/news/253344]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些模型的细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制(Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在seq2seq网络中的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "v = [random.gauss(0,1) for z in range(40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([i for i in range(12)])\n",
    "a = a.reshape((6,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  5,  7,  9, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a[:,1]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 0, 5, 1, 4, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.random.randint(0,10,(6,))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1,  3,  5,  7,  9, 11]), array([7, 0, 5, 1, 4, 3])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = []\n",
    "reward.append(b)\n",
    "reward.append(d)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-3e6e4dc750fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "reward.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,12):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "397.313px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "331.667px",
    "left": "1199.33px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
