{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vscode\n",
    "\n",
    "1. 查看函数或者类的定义\n",
    "`Ctrl`+`鼠标左键`点击函数名或者类名即可跳转到定义处，在函数名或者类名上按`F12`也可以实现同样功能\n",
    "\n",
    "\n",
    "2. 命名重构：\n",
    "在变量名上按`F2`即可实现重命名变量\n",
    "\n",
    "\n",
    "3. 方法重构:\n",
    "选中某一段代码，这个时候，代码的左侧会出现一个「灯泡图标」，点击这个图标，就可以把这段代码提取为一个单独的函数\n",
    "\n",
    "\n",
    "4. python断点调试:\n",
    "在行号的左边点击即可设置断点，在左边的调试界面可以查看变量的变化\n",
    "\n",
    "\n",
    "5. 函数在哪被调用了：\n",
    "选中`函数`（或者将光标放置在`函数`上），然后按住快捷键「Shift + F12」，就能看到`函数`在哪些地方被调用了，比较实用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jupyter notebook\n",
    "\n",
    "- 恢复原来写过的代码:\n",
    "\n",
    "  场景：在某个窗口写了很多代码，又删除了很多单元格，想找回原来的代码。\n",
    "  \n",
    "  解决方法：直接在一个单元格中写入`history`就会展示出历史代码（前提是你运行过的，否则不会打印出来）\n",
    "  \n",
    "- Move selected cells\n",
    "\n",
    "  Move selected cell*s* using keybaord shortcuts `Alt + up` and `Alt + down` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## win10相关\n",
    "\n",
    "- Sticky Note：Go to the Windows Ink Workspace > Sticky Notes to create reminders for yourself. \n",
    "\n",
    "- Stay focused：Select and hold the window you want to stay open, then give your mouse (or finger) a little back-and-forth shake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\r和\\n\n",
    "`\\r`是回车，`\\n`是换行，前者使光标到行首，后者使光标下移一格。\n",
    "\n",
    "通常用的Enter是两个加起来的，即`\\r\\n`，也就是说`\\r\\n`算两个字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## str.strip()和str.split()\n",
    "- 按某一个字符分割，如‘.':\n",
    "```python\n",
    ">>> str = ('www.google.com')\n",
    ">>> print str\n",
    "www.google.com\n",
    ">>> str_split = str.split('.')\n",
    ">>> print(str_split) # 得到的结果是一个list\n",
    "['www', 'google', 'com']\n",
    "```\n",
    "\n",
    "- 按某一个字符分割，且分割n次。如按‘.'分割1次\n",
    "```python\n",
    ">>> str_split = str.split('.',1)\n",
    ">>> print str_split\n",
    "['www', 'google.com']\n",
    "```\n",
    "\n",
    "- split分隔后是一个列表，[0]表示取其第一个元素： \n",
    "```python\n",
    ">>> str_split = str.split('.')[0]\n",
    ">>> print str_split\n",
    "www\n",
    "```\n",
    "> split()函数后面还可以加正则表达式\n",
    "\n",
    "------\n",
    "- **我曾经用这两句取出str中的前200个单词（单词是用空格隔开的），并从新组成str**\n",
    "```python\n",
    "text = str.lower().split(' ')[:200]\n",
    "text = ' '.join(text) # 一个字符串列表（列表的元素是字符串）变成一个字符串\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## str.title():把字符串变成标题的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Awesome Python Tricks\n"
     ]
    }
   ],
   "source": [
    "my_string = \"10 awesome python tricks\"\n",
    "print(my_string.title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch的一些知识\n",
    "\n",
    "## 在理解模型时一些有效的方法\n",
    "\n",
    "### net.parameters( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_emb, emb_dim, hidden_dim, use_cuda):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_emb = num_emb\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_cuda = use_cuda\n",
    "        self.emb = nn.Embedding(num_emb, emb_dim) \n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True) \n",
    "        self.lin = nn.Linear(hidden_dim, num_emb)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        h0, c0 = self.init_hidden(x.size(0))\n",
    "        output, (h, c) = self.lstm(emb, (h0, c0)) \n",
    "        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim))) \n",
    "        return pred\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        c = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        if self.use_cuda:\n",
    "            h, c = h.cuda(), c.cuda()\n",
    "        return h, c\n",
    "    \n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "generator = Generator(num_emb = 5000,emb_dim = 128,hidden_dim = 64,use_cuda = 'Ture')\n",
    "generator = generator.cuda()\n",
    "x = torch.LongTensor([[2,50,100],\n",
    "                      [40,3,1000]]).cuda()\n",
    "pred = generator(x)\n",
    "params = list(generator.parameters())\n",
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (emb): Embedding(5000, 128)\n",
      "  (lstm): LSTM(128, 64, batch_first=True)\n",
      "  (lin): Linear(in_features=64, out_features=5000, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Examining a Model's Structure\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.weight : torch.Size([5000, 128])\n",
      "lstm.weight_ih_l0 : torch.Size([256, 128])\n",
      "lstm.weight_hh_l0 : torch.Size([256, 64])\n",
      "lstm.bias_ih_l0 : torch.Size([256])\n",
      "lstm.bias_hh_l0 : torch.Size([256])\n",
      "lin.weight : torch.Size([5000, 64])\n",
      "lin.bias : torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in generator.named_parameters():\n",
    "    print(name,':',parameters.size())\n",
    "# 需要值的话:parameters.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.4876, -8.4922, -8.5338,  ..., -8.5332, -8.5663, -8.5036],\n",
       "        [-8.4900, -8.4903, -8.5346,  ..., -8.5317, -8.5671, -8.5047],\n",
       "        [-8.4903, -8.4888, -8.5338,  ..., -8.5304, -8.5693, -8.5053],\n",
       "        [-8.4885, -8.4912, -8.5348,  ..., -8.5337, -8.5656, -8.5034],\n",
       "        [-8.4893, -8.4897, -8.5355,  ..., -8.5316, -8.5680, -8.5050],\n",
       "        [-8.4893, -8.4881, -8.5349,  ..., -8.5301, -8.5694, -8.5057]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对于随处可见的retain_graph参数\n",
    "\n",
    "如果retain_graph=true,就会每次运行时重新生成图。也就是说，每次 backward() 时，默认会把整个计算图free掉。一般情况下是每次迭代，只需一次 forward() 和一次 backward() , 前向运算forward() 和反向传播backward()是成对存在的，一般一次backward()也是够用的。但是不排除，由于自定义loss等的复杂性，需要一次forward()，多个不同loss的backward()来累积同一个网络的grad来更新参数。于是，若在当前backward()后，不执行forward() 而可以执行另一个backward()，需要在当前backward()时，指定保留计算图，即backward(retain_graph)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([-0.4168], requires_grad=True) \n",
      " x.grad =  None \n",
      "\n",
      "x's grad after the first opertion:\n",
      " tensor([2.]) \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2a33cd089ef1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x's grad after the first opertion:\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x's grad:\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# buffers 是什么，这里可以了解retain_graph的用法\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(1, requires_grad=True)\n",
    "print(\"x = \",x,\"\\n\",\n",
    "     \"x.grad = \",x.grad,\"\\n\")\n",
    "y = x*2\n",
    "y.backward()\n",
    "print(\"x's grad after the first opertion:\\n\",x.grad,\"\\n\")\n",
    "y.backward()\n",
    "print(\"x's grad:\\n\",x.grad,\"\\n\")\n",
    "# buffers 是什么，这里可以了解retain_graph的用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了了解上述Error里面的`buffer`这个词，可以参照官网在Backprop章节的这段话：\n",
    "> To backpropagate the error all we have to do is to `loss.backward()`. You need to clear the existing gradients though(use`net.zero_grad()`to zeroes the gradient **buffers** of all parameters), else gradients will be accumulated to existing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor.detach()\n",
    "\n",
    "如果 x 为中间输出，y = x.detach 表示创建一个与 x 相同，但requires_grad==False 的tensor, 实际上就是把y 以前的计算图 grad_fn 都消除，y自然也就成了叶节点。原先反向传播时，回传到x时还会继续，而现在回到y处后，就结束了，不继续回传求到了。另外值得注意, x和y指向同一个Tensor ,即 x.data 。而detach_() 表示不创建新变量，而是直接修改 x 本身。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.functional.pad(input, pad, mode='constant', value=0)\n",
    "> Pads tensor 我只试验了2维的pad\n",
    "\n",
    "* 格式：torch.nn.functional.pad(input, pad, mode='constant', value=0)\n",
    "  - **input (Tensor)** – 2-dimensional tensor\n",
    "  - **pad (tuple)** – 2-elements tuple,我只试验了;`(0，i)`\n",
    "  - **mode** - 我只用了Defaull:`constant`\n",
    "  - **value** - fill value for padding.Default:`0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  tensor([[2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2]]) \n",
      "a.size =  torch.Size([3, 5]) \n",
      "\n",
      "b =  tensor([[2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "        [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "        [2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]) \n",
      "b.size =  torch.Size([3, 20]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = Variable(nn.init.constant_(torch.zeros(3, 5), 2)).long()\n",
    "print(\"a = \",a,\"\\n\"\n",
    "     \"a.size = \",a.size(),\"\\n\")\n",
    "b = nn.functional.pad(a, (0,20 - 5), value=3)\n",
    "print(\"b = \",b,\"\\n\"\n",
    "     \"b.size = \",b.size(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.init.constant_()\n",
    "\n",
    "> Fills the input Tensor with the value val.\n",
    "\n",
    "* 格式：torch.nn.init.constant_(tensor, val)\n",
    "  - **tensor** – an n-dimensional torch.Tensor\n",
    "  - **val** – the value to fill the tensor with\n",
    "\n",
    "* Examples\n",
    "```python\n",
    ">>> w = torch.empty(3, 5)\n",
    ">>> nn.init.constant_(w, 0.3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorDataset() &DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 |Step: 0 |batch_x: [5. 7. 1.] |batch_y: [ 6.  4. 10.]\n",
      "Epoch: 0 |Step: 1 |batch_x: [ 4. 10.  2.] |batch_y: [7. 1. 9.]\n",
      "Epoch: 0 |Step: 2 |batch_x: [6. 9. 3.] |batch_y: [5. 2. 8.]\n",
      "Epoch: 0 |Step: 3 |batch_x: [8.] |batch_y: [3.]\n",
      "Epoch: 1 |Step: 0 |batch_x: [5. 9. 2.] |batch_y: [6. 2. 9.]\n",
      "Epoch: 1 |Step: 1 |batch_x: [4. 6. 8.] |batch_y: [7. 5. 3.]\n",
      "Epoch: 1 |Step: 2 |batch_x: [1. 3. 7.] |batch_y: [10.  8.  4.]\n",
      "Epoch: 1 |Step: 3 |batch_x: [10.] |batch_y: [1.]\n",
      "Epoch: 2 |Step: 0 |batch_x: [4. 7. 9.] |batch_y: [7. 4. 2.]\n",
      "Epoch: 2 |Step: 1 |batch_x: [6. 2. 3.] |batch_y: [5. 9. 8.]\n",
      "Epoch: 2 |Step: 2 |batch_x: [1. 5. 8.] |batch_y: [10.  6.  3.]\n",
      "Epoch: 2 |Step: 3 |batch_x: [10.] |batch_y: [1.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(1,10,10) # linspace(star,end,step):从1到10，10个step走完\n",
    "y = torch.linspace(10,1,10)\n",
    "\n",
    "import torch.utils.data as Data\n",
    "torch_dataset = Data.TensorDataset(x,y)\n",
    "loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = 3,\n",
    "    shuffle = True,\n",
    "    num_workers = 4)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for step,(batch_x,batch_y) in enumerate(loader):\n",
    "        print('Epoch:',epoch,'|Step:',step,'|batch_x:',batch_x.numpy(),\n",
    "             '|batch_y:',batch_y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor.detach()\n",
    "\n",
    "假设有模型A和模型B，我们需要将A的输出作为B的输入，但训练时我们只训练模型B. 那么可以这样做：\n",
    "```python\n",
    "input_B = output_A.detach()\n",
    "```\n",
    "它可以使两个计算图的梯度传递断开，从而实现我们所需的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.Dropout(p=0.5, inplace=False)\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.\n",
    "\n",
    "Furthermore, the outputs are scaled by a factor of $\\frac{1}{1-p}$ during training. This means that during evaluation the module simply computes an identity function.\n",
    "\n",
    "* Parameters:\n",
    "  - **p** – probability of an element to be zeroed. Default: 0.5\n",
    "  - **inplace** – If set to True, will do this operation in-place. Default: False\n",
    "* Shape:\n",
    "  - Input: (*).Input can be of any shape\n",
    "  - Output: (*).Output is of the same shape as input\n",
    "  \n",
    "> **NOTE:**\n",
    "`Dropout` should take place only during training. If it was happening during inference time, you'd lose a chunk of your network's reasoning power, which is not what we want! Thankfully, PyTorch's implementation of `Dropout` works out which mode you're running in and passes all the data through the Dropout layer at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4651, -1.5681, -0.1346,  0.3834,  0.5632],\n",
      "        [ 0.6371, -0.0706,  1.1372,  0.7815, -1.2001],\n",
      "        [ 0.5383,  0.5353, -0.2802, -0.9498, -1.0700],\n",
      "        [ 1.2053, -0.4515,  0.7759, -0.8358,  0.1974]]) \n",
      " tensor([[-0.5814, -0.0000, -0.1683,  0.0000,  0.7040],\n",
      "        [ 0.7964, -0.0883,  1.4215,  0.9769, -1.5001],\n",
      "        [ 0.6728,  0.6692, -0.0000, -1.1872, -1.3375],\n",
      "        [ 0.0000, -0.0000,  0.9699, -1.0447,  0.2467]]) \n",
      " tensor([[-0.5814, -1.9601, -0.1683,  0.4793,  0.7040],\n",
      "        [ 0.7964, -0.0883,  1.4215,  0.9769, -1.5001],\n",
      "        [ 0.6728,  0.6692, -0.3502, -1.1872, -1.3375],\n",
      "        [ 1.5066, -0.5644,  0.9699, -1.0447,  0.2467]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input_1 = torch.randn(4, 5)\n",
    "output_1 = m(input_1)\n",
    "output_2 = input_1 * (1 / (1 - 0.2))\n",
    "print(input_1,'\\n',output_1,'\\n',output_2) #注意output_1中不是0的数字都和output_2一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.Conv2d()\n",
    "**Parameters:**\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "注意输入维度必须是4维：batch_size * in_channels * x * y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.CrossEntropyLoss()和nn.NLLLoss()\n",
    "\n",
    "- **Softmax Activation Function:**\n",
    "\n",
    "$$S(f_{y_i}) = \\dfrac{e^{f_{y_i}}}{\\sum_{j}e^{f_j}}$$\n",
    "\n",
    "> In practice, the softmax function is used with the `negative log-likelihood (NLL)`\n",
    "\n",
    "- **NLLLoss(negative log likelihood loss):**\n",
    "\n",
    "$$L(\\mathbf{y}) = -\\log(\\mathbf{y})$$\n",
    "\n",
    "> 注意到$ y=-log(x) $函数在[0,1]上是单调减函数，所以自变量越小，loss越大；自变量越大，loss越小。所以higher confidence(which come from the Softmax Activation Function) at the correct class leads to lower loss and vice versa.\n",
    "\n",
    "- **CrossEntropyLoss:**\n",
    "\n",
    "$ Y = (y_1,y_2,...) $是target，而且是one-hot标签(即0或者1，这里是数学运算表示，注意代码的taget是一个比$C-1$小的整数就可以了)，$ P = (P_1,P_2,...) $是经过Softmax层输出的pred\n",
    "\n",
    "$$ E = -\\sum_{1}^{n}y_{i} * log(P_{i})$$ \n",
    "\n",
    "**以下是pytorch代码解释，注意数学公式和代码表示的区别：**\n",
    "\n",
    "CrossEntropyLoss() = log_softmax() + NLLLoss()：\n",
    "> This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
    "\n",
    "$$CrossEntropyLoss(X,class) = - log\\frac{e^{x_{class}}}{\\sum e^{x_i}}$$\n",
    "\n",
    "> input:(N,C),N = Batch_size,C = num_classes #其中每个datapoint也就是每一横排，就是上面的X\n",
    "\n",
    "> Target:(N)  #一定要注意这不是(N,1)，而是(N,)，对应 target.view(-1)，而且这里不是one-hot标签，这里满足$ 0 \\leq target[i]\\leq C-1$\n",
    "\n",
    "> Output:scalar # 只和相应的$ x_{class}$大小相关,因为从数学表达式中我们可以看到其他的$y_{i}$是等于0的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0205)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.LongTensor([1,0,0,1]) #能不能出现除了0、1之外的数字\n",
    "pred = torch.Tensor([-988,-0.01,-0.0005,-1080,-0.009,-3880,-180,-0.001]).view(4,2)\n",
    "# 上面改成pred = torch.Tensor([-98,-0.01,-0.0005,-100,-0.009,-300,-10,-0.001]).view(4,2)结果一样\n",
    "dis_criterion = nn.NLLLoss(reduction='sum')\n",
    "a = dis_criterion(pred,target)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.8800e+02, -1.0000e-02],\n",
       "        [-5.0000e-04, -1.0800e+03],\n",
       "        [-9.0000e-03, -3.8800e+03],\n",
       "        [-1.8000e+02, -1.0000e-03]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0444,  0.0298,  0.4169,  1.1406, -0.8716],\n",
       "        [-1.2420,  0.8809, -1.2497, -0.5093,  0.1507],\n",
       "        [ 0.1278, -1.8586, -0.4689,  0.1224, -0.0527]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input1 = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5) # 应为每一行有5个预测值\n",
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4608, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = loss(input1,target)\n",
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 针对target，我修改一下input1，使得loss变小\n",
    "input1[0][2],input1[1][4],input1[2][4] = 10,10,10\n",
    "bbb = loss(input1,target)\n",
    "bbb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.LSTM & torch.nn.LSTMCell \n",
    "\n",
    "* **Inputs**: input, (h_0, c_0)\n",
    "  - `input` of shape (seq_len, batch, input_size)\n",
    "  - `h_0` of shape (num_layers * num_directions, batch, hidden_size)\n",
    "  - `c_0` of shape (num_layers * num_directions, batch, hidden_size)\n",
    "  - If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.\n",
    "* **Outputs**: output, (h_n, c_n)\n",
    "  - `output` of shape (seq_len, batch, num_directions * hidden_size)\n",
    "  - `h_n` of shape (num_layers * num_directions, batch, hidden_size)\n",
    "  - `c_n` of shape (num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 20])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个input_size = 1 ， hidden_size = 20 ，num_layers = 1\n",
    "rnn = nn.LSTM(10,20,1,batch_first = True) # batch_first不影响h和c,影响output和input\n",
    "input_1 = torch.randn(3, 5, 10)\n",
    "h0 = torch.randn(1, 3, 20)\n",
    "c0 = torch.randn(1, 3, 20)\n",
    "output, (hn, cn) = rnn(input_1, (h0, c0))\n",
    "output.size() #[batch_size,seq_len,feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么RNN类都有默认batch_size不排第一的反人类设定呢？我觉得神书第六章的代码片段可以解释（因为方便用RNNcell)：\n",
    "```python\n",
    "class ElmanRNN(nn.Module):\n",
    "    \"\"\" an Elman RNN built using the RNNCell \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): size of the input vectors\n",
    "            hidden_size (int): size of the hidden state vectors\n",
    "            bathc_first (bool): whether the 0th dimension is batch\n",
    "        \"\"\"\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        \n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def _initial_hidden(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, x_in, initial_hidden=None):\n",
    "        \"\"\"The forward pass of the ElmanRNN\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                If self.batch_first: x_in.shape = (batch, seq_size, feat_size)\n",
    "                Else: x_in.shape = (seq_size, batch, feat_size)\n",
    "            initial_hidden (torch.Tensor): the initial hidden state for the RNN\n",
    "        Returns:\n",
    "            hiddens (torch.Tensor): The outputs of the RNN at each time step. \n",
    "                If self.batch_first: hiddens.shape = (batch, seq_size, hidden_size)\n",
    "                Else: hiddens.shape = (seq_size, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "        # 这个if&else不管三七二十一，先把x_size的格式先变成seq_size, batch_size, feat_size\n",
    "        hiddens = []\n",
    "\n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self._initial_hidden(batch_size)\n",
    "            initial_hidden = initial_hidden.to(x_in.device)\n",
    "\n",
    "        hidden_t = initial_hidden\n",
    "                    \n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t) #这里就是为什么batch_size不排第一的原因吧\n",
    "            hiddens.append(hidden_t)\n",
    "            \n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.multinomial()\n",
    "\n",
    "> torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor\n",
    "\n",
    "```python\n",
    ">>> weights = torch.Tensor([0, 10, 3, 0]) # create a Tensor of weights\n",
    ">>> torch.multinomial(weights, 4) #可以试试重复运行这条命令，发现只会有2种结果：[1 2 0 0]以及[2 1 0 0]，以[1 2 0 0]这种情况居多。\n",
    " 1\n",
    " 2\n",
    " 0\n",
    " 0\n",
    "[torch.LongTensor of size 4]\n",
    " \n",
    ">>> torch.multinomial(weights, 4, replacement=True)\n",
    " 1\n",
    " 2\n",
    " 1\n",
    " 2\n",
    "[torch.LongTensor of size 4]\n",
    "```\n",
    "- input张量可以看成一个权重张量，每一个元素代表其在该行中的权重。如果有元素为0，那么在其他不为0的元素被取干净之前，这个元素是不会被取到的。\n",
    "- n_samples是每一行的取值次数，该值不能大于每一样的元素数，否则会报错。\n",
    "- replacement指的是取样时是否是有放回的取样，True是有放回，False无放回。\n",
    "- 输入二维张量，则返回的也会成为一个二维张量，行数为输入的行数，列数为n_samples，即每一行都取了n_samples次，取法和一维张量相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单行条件语句\n",
    "\n",
    "**格式：**`[on_true] if [expression] else [on_false]`\n",
    "\n",
    "**例如：**\n",
    "```python\n",
    "x = \"Success!\" if (y==2) else \"Failed!\"\n",
    "```\n",
    "------\n",
    "**也可以多个判断：**\n",
    "```python\n",
    "x = int(input())\n",
    "if x >= 10:\n",
    "    print(\"horse\")\n",
    "elif 1 < x < 10:\n",
    "    print(\"Duck\")\n",
    "else:\n",
    "    print(\"other\")\n",
    "```\n",
    "上面的代码一行可以写完：\n",
    "\n",
    "`print('horse' if x >= 10 else \"Duck\" if 1 < x < 10 else \"other\")`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assert函数\n",
    "\n",
    "* 格式：assert expression [, arguments]\n",
    "\n",
    "```python\n",
    ">>> assert 1==2, '1 不等于 2'\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "AssertionError: 1 不等于 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\*args和\\**kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 (7, 8, 9) {'a': 1, 'b': 2, 'c': 3}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " *args 用来将参数打包成tuple给函数体调用\n",
    " **kwargs 打包关键字参数成dict给函数体调用\n",
    "'''\n",
    "def function(arg,*args,**kwargs):\n",
    "    print(arg,args,kwargs)\n",
    "\n",
    "function(6,7,8,9,a=1, b=2, c=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enumerate(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 j\n",
      "1 i\n",
      "2 a\n",
      "3 n\n"
     ]
    }
   ],
   "source": [
    "surname = \"jian\" \n",
    "for position_index, character in enumerate(surname):\n",
    "    print(position_index,character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同的tpye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同type之间的转换\n",
    "\n",
    "- **Tensor与Numpy Array之间的转换：**\n",
    "\n",
    "  Tensor----> Numpy  可以使用 data.numpy()，data为Tensor变量\n",
    "\n",
    "  Numpy ----> Tensor 可以使用 torch.from_numpy(data)，data为numpy变量\n",
    "  \n",
    "  \n",
    "\n",
    "- **List类型与numpy.array类型的互相转换：**\n",
    "\n",
    "  temp = np.array(list) \n",
    "\n",
    "  arr = temp.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "new_array_1 = np.array([1,2, 3,4, 5,6])\n",
    "new_array_2 = np.array([2, 3,4, 5,6,7])\n",
    "tensor_1 = torch.from_numpy(new_array_1)\n",
    "tensor_2 = torch.from_numpy(new_array_2)\n",
    "tensor_1.numpy()[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list类（一些字符串型在“文本处理”部分）\n",
    "\n",
    "### 列表表达式\n",
    "```python\n",
    "[ expression for item in list if conditional ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mylist= [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \n",
      " squares= [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] \n",
      " my_formula= [2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0] \n",
      "filtered= [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    }
   ],
   "source": [
    "mylist = [i for i in range(10)]\n",
    "squares = [x**2 for x in range(10)]\n",
    "def my_function(a):\n",
    "    return (a + 5) / 2\n",
    "my_formula = [my_function(x) for x in range(10)]\n",
    "filtered = [x for x in range(20) if x%2==0]\n",
    "print('mylist=',mylist,'\\n',\n",
    "     'squares=',squares,'\\n',\n",
    "     'my_formula=',my_formula,'\\n'\n",
    "      'filtered=',filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list切片\n",
    "```python\n",
    "a[start:stop:step]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rev_string= gfedcba \n",
      "\r",
      " rev_array= [5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "rev_string = \"abcdefg\"[::-1]\n",
    "rev_array = [1,2,3,4,5][::-1]\n",
    "print(\"rev_string=\",rev_string,\"\\n\\r\",\n",
    "     \"rev_array=\",rev_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计函数：set(list),max(list),list.count,Counter(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5, 6}\n"
     ]
    }
   ],
   "source": [
    "list_test = [1,1,2,3,4,5,5,5,6,6]\n",
    "print(set(list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list.count是list的内置函数\n",
    "list_test.count(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_test.count(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({5: 3, 1: 2, 6: 2, 2: 1, 3: 1, 4: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(list_test) # 显然可以用在str上统计字符个数\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map()函数\n",
    "\n",
    "* 语法：\n",
    "  `map(function, iterable, ...)`\n",
    "* 描述：第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表；\n",
    "* 参数：\n",
    "    - function --> 函数\n",
    "    - iterable --> 一个或多个序列\n",
    "* 返回值：迭代器\n",
    "* 实例：\n",
    "\n",
    "```python\n",
    ">>>def square(x) :            # 计算平方数\n",
    "...     return x ** 2\n",
    "\n",
    ">>> map(square, [1,2,3,4,5])   # 计算列表各个元素的平方\n",
    "out:[1, 4, 9, 16, 25]\n",
    "```\n",
    "------\n",
    "\n",
    "```python\n",
    ">>> map(lambda x: x ** 2, [1, 2, 3, 4, 5])  # 使用 lambda 匿名函数\n",
    "out:[1, 4, 9, 16, 25]\n",
    " \n",
    "# 提供了两个列表，对相同位置的列表数据进行相加\n",
    ">>> map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])\n",
    "out:[3, 7, 11, 15, 19]\n",
    "\n",
    ">>> def upper(s):\n",
    "    return s.upper()\n",
    "\n",
    ">>> mylist = list(map(upper,['sentence','fragment']))\n",
    ">>> print(mylist)\n",
    "out:['SENTENCE','FRAGMENT']\n",
    "\n",
    "# Convert a string representation of a number into a list of ints.\n",
    ">>> list_of_ints = list(map(int,\"123456\")))\n",
    ">>> print(list_of_ints)\n",
    "out: [1,2,3,4,5,6]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip()函数\n",
    "\n",
    "```python\n",
    ">>> a = [1,2,3]\n",
    ">>> b = [4,5,6]\n",
    ">>> c = [4,5,6,7,8]\n",
    ">>> zipped = zip(a,b) # 返回一个对象\n",
    ">>> zipped\n",
    "<zip object at 0x103abc288>\n",
    ">>> list(zipped)  # list() 转换为列表\n",
    "[(1, 4), (2, 5), (3, 6)]\n",
    ">>> list(zip(a,c))  # 元素个数与最短的列表一致\n",
    "[(1, 4), (2, 5), (3, 6)]\n",
    " \n",
    ">>> a1, a2 = zip(*zip(a,b)) # 与 zip 相反，zip(*) 可理解为解压，返回二维矩阵式\n",
    ">>> list(a1)\n",
    "[1, 2, 3]\n",
    ">>> list(a2)\n",
    "[4, 5, 6]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据类（python3.7开始支持）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card.rank= Q \n",
      " \n",
      " card= Card(rank='Q', suit='hearts')\n"
     ]
    }
   ],
   "source": [
    "# 具体详见：https://realpython.com/python-data-classes/\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Card:\n",
    "    rank:str\n",
    "    suit:str\n",
    "        \n",
    "card = Card(\"Q\",\"hearts\")\n",
    "\n",
    "print('card.rank=',card.rank,'\\n',\n",
    "      '\\n',\n",
    "     'card=',card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dict类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google www.google.com\n",
      "taobao www.taobao.com\n",
      "Runoob www.runoob.com\n"
     ]
    }
   ],
   "source": [
    "my_dict =  {'Google': 'www.google.com', 'taobao': 'www.taobao.com', 'Runoob': 'www.runoob.com'}\n",
    "for key,value in my_dict.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 3, 'c': 4}\n"
     ]
    }
   ],
   "source": [
    "dict1 = {'a':1,'b':2}\n",
    "dict2 = {'b':3,'c':4}\n",
    "merged = { **dict1,**dict2 } # 如果有重复的key，那么第一个词典的这个key对应的value会被覆盖掉\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些模型细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## residual connection\n",
    "```python\n",
    "# 来自《The Annotated Transformer》by harvestnlp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text Data Augmentation \n",
    "来自<EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks> in 2019\n",
    "\n",
    "* three augmentation strategies: \n",
    "  - random insertion\n",
    "  - random swap\n",
    "  - random deletion\n",
    "  \n",
    "> The techniques in the EDA paper average about a 3% improvement in accuracy when used with small amounts of labeled examples (roughly 500). If you have more than 5,000 examples in your dataset, the paper suggests that this improvement may fall to 0.8% or lower, due to the model obtaining better generalization from the larger amounts of data available over the improvements that EDA can provide.\n",
    "\n",
    "* Back Translation\n",
    "\n",
    "```python\n",
    "pip install googletrans\n",
    "# Then, we can translate our sentence from English to French, and then back to English\n",
    "import googletrans\n",
    "import googletrans.Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "sentences = ['The cat sat on the mat']\n",
    "\n",
    "translation_fr = translator.translate(sentences, dest='fr')\n",
    "fr_text = [t.text for t in translations_fr]\n",
    "translation_en = translator.translate(fr_text, dest='en')\n",
    "en_text = [t.text for t in translation_en]\n",
    "print(en_text)\n",
    "\n",
    "[out]:['The cat sat on the carpet']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "278.26px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
