{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vscode\n",
    "\n",
    "1. 查看函数或者类的定义\n",
    "`Ctrl`+`鼠标左键`点击函数名或者类名即可跳转到定义处，在函数名或者类名上按`F12`也可以实现同样功能\n",
    "\n",
    "\n",
    "2. 命名重构：\n",
    "在变量名上按`F2`即可实现重命名变量\n",
    "\n",
    "\n",
    "3. 方法重构:\n",
    "选中某一段代码，这个时候，代码的左侧会出现一个「灯泡图标」，点击这个图标，就可以把这段代码提取为一个单独的函数\n",
    "\n",
    "\n",
    "4. python断点调试:\n",
    "在行号的左边点击即可设置断点，在左边的调试界面可以查看变量的变化\n",
    "\n",
    "\n",
    "5. 函数在哪被调用了：\n",
    "选中`函数`（或者将光标放置在`函数`上），然后按住快捷键「Shift + F12」，就能看到`函数`在哪些地方被调用了，比较实用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jupyter notebook\n",
    "\n",
    "- 恢复原来写过的代码:\n",
    "\n",
    "  场景：在某个窗口写了很多代码，又删除了很多单元格，想找回原来的代码。\n",
    "  \n",
    "  解决方法：直接在一个单元格中写入`history`就会展示出历史代码（前提是你运行过的，否则不会打印出来）\n",
    "  \n",
    "- Move selected cells\n",
    "\n",
    "  Move selected cell*s* using keybaord shortcuts `Alt + up` and `Alt + down` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## win10相关\n",
    "\n",
    "- Sticky Note：Go to the Windows Ink Workspace > Sticky Notes to create reminders for yourself. \n",
    "\n",
    "- Stay focused：Select and hold the window you want to stay open, then give your mouse (or finger) a little back-and-forth shake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\r和\\n\n",
    "`\\r`是回车，`\\n`是换行，前者使光标到行首，后者使光标下移一格。\n",
    "\n",
    "通常用的Enter是两个加起来的，即`\\r\\n`，也就是说`\\r\\n`算两个字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## str.strip()和str.split()\n",
    "- 按某一个字符分割，如‘.':\n",
    "```python\n",
    ">>> str = ('www.google.com')\n",
    ">>> print str\n",
    "www.google.com\n",
    ">>> str_split = str.split('.')\n",
    ">>> print(str_split) # 得到的结果是一个list\n",
    "['www', 'google', 'com']\n",
    "```\n",
    "\n",
    "- 按某一个字符分割，且分割n次。如按‘.'分割1次\n",
    "```python\n",
    ">>> str_split = str.split('.',1)\n",
    ">>> print str_split\n",
    "['www', 'google.com']\n",
    "```\n",
    "\n",
    "- split分隔后是一个列表，[0]表示取其第一个元素： \n",
    "```python\n",
    ">>> str_split = str.split('.')[0]\n",
    ">>> print str_split\n",
    "www\n",
    "```\n",
    "> split()函数后面还可以加正则表达式\n",
    "\n",
    "------\n",
    "- **我曾经用这两句取出str中的前200个单词（单词是用空格隔开的），并从新组成str**\n",
    "```python\n",
    "text = str.lower().split(' ')[:200]\n",
    "text = ' '.join(text) # 一个字符串列表（列表的元素是字符串）变成一个字符串\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## str.title():把字符串变成标题的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Awesome Python Tricks\n"
     ]
    }
   ],
   "source": [
    "my_string = \"10 awesome python tricks\"\n",
    "print(my_string.title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch的一些知识\n",
    "\n",
    "## 在理解模型时一些有效的方法\n",
    "\n",
    "### net.parameters( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_emb, emb_dim, hidden_dim, use_cuda):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_emb = num_emb\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_cuda = use_cuda\n",
    "        self.emb = nn.Embedding(num_emb, emb_dim) \n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True) \n",
    "        self.lin = nn.Linear(hidden_dim, num_emb)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        h0, c0 = self.init_hidden(x.size(0))\n",
    "        output, (h, c) = self.lstm(emb, (h0, c0)) \n",
    "        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim))) \n",
    "        return pred\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        c = Variable(torch.zeros((1, batch_size, self.hidden_dim)))\n",
    "        if self.use_cuda:\n",
    "            h, c = h.cuda(), c.cuda()\n",
    "        return h, c\n",
    "    \n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "generator = Generator(num_emb = 5000,emb_dim = 128,hidden_dim = 64,use_cuda = 'Ture')\n",
    "generator = generator.cuda()\n",
    "x = torch.LongTensor([[2,50,100],\n",
    "                      [40,3,1000]]).cuda()\n",
    "pred = generator(x)\n",
    "params = list(generator.parameters())\n",
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (emb): Embedding(5000, 128)\n",
      "  (lstm): LSTM(128, 64, batch_first=True)\n",
      "  (lin): Linear(in_features=64, out_features=5000, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Examining a Model's Structure\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.weight : torch.Size([5000, 128])\n",
      "lstm.weight_ih_l0 : torch.Size([256, 128])\n",
      "lstm.weight_hh_l0 : torch.Size([256, 64])\n",
      "lstm.bias_ih_l0 : torch.Size([256])\n",
      "lstm.bias_hh_l0 : torch.Size([256])\n",
      "lin.weight : torch.Size([5000, 64])\n",
      "lin.bias : torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in generator.named_parameters():\n",
    "    print(name,':',parameters.size())\n",
    "# 需要值的话:parameters.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.4876, -8.4922, -8.5338,  ..., -8.5332, -8.5663, -8.5036],\n",
       "        [-8.4900, -8.4903, -8.5346,  ..., -8.5317, -8.5671, -8.5047],\n",
       "        [-8.4903, -8.4888, -8.5338,  ..., -8.5304, -8.5693, -8.5053],\n",
       "        [-8.4885, -8.4912, -8.5348,  ..., -8.5337, -8.5656, -8.5034],\n",
       "        [-8.4893, -8.4897, -8.5355,  ..., -8.5316, -8.5680, -8.5050],\n",
       "        [-8.4893, -8.4881, -8.5349,  ..., -8.5301, -8.5694, -8.5057]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 |Step: 0 |batch_x: [7. 8. 6. 1. 4.] |batch_y: [ 4.  3.  5. 10.  7.]\n",
      "Epoch: 0 |Step: 1 |batch_x: [ 2.  5.  9.  3. 10.] |batch_y: [9. 6. 2. 8. 1.]\n",
      "Epoch: 1 |Step: 0 |batch_x: [ 1. 10.  2.  5.  4.] |batch_y: [10.  1.  9.  6.  7.]\n",
      "Epoch: 1 |Step: 1 |batch_x: [9. 3. 8. 7. 6.] |batch_y: [2. 8. 3. 4. 5.]\n",
      "Epoch: 2 |Step: 0 |batch_x: [ 4.  7.  3. 10.  2.] |batch_y: [7. 4. 8. 1. 9.]\n",
      "Epoch: 2 |Step: 1 |batch_x: [6. 8. 9. 1. 5.] |batch_y: [ 5.  3.  2. 10.  6.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(1,10,10) # linspace(star,end,step):从1到10，10个step走完\n",
    "y = torch.linspace(10,1,10)\n",
    "\n",
    "import torch.utils.data as Data\n",
    "torch_dataset = Data.TensorDataset(x,y)\n",
    "loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 4)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for step,(batch_x,batch_y) in enumerate(loader):\n",
    "        print('Epoch:',epoch,'|Step:',step,'|batch_x:',batch_x.numpy(),\n",
    "             '|batch_y:',batch_y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor.detach()\n",
    "\n",
    "假设有模型A和模型B，我们需要将A的输出作为B的输入，但训练时我们只训练模型B. 那么可以这样做：\n",
    "```python\n",
    "input_B = output_A.detach()\n",
    "```\n",
    "它可以使两个计算图的梯度传递断开，从而实现我们所需的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS  torch.nn.Dropout(p=0.5, inplace=False)\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.\n",
    "\n",
    "Furthermore, the outputs are scaled by a factor of $\\frac{1}{1-p}$ during training. This means that during evaluation the module simply computes an identity function.\n",
    "\n",
    "* Parameters:\n",
    "  - **p** – probability of an element to be zeroed. Default: 0.5\n",
    "  - **inplace** – If set to True, will do this operation in-place. Default: False\n",
    "* Shape:\n",
    "  - Input: (*).Input can be of any shape\n",
    "  - Output: (*).Output is of the same shape as input\n",
    "  \n",
    "> **NOTE:**\n",
    "`Dropout` should take place only during training. If it was happening during inference time, you'd lose a chunk of your network's reasoning power, which is not what we want! Thankfully, PyTorch's implementation of `Dropout` works out which mode you're running in and passes all the data through the Dropout layer at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4651, -1.5681, -0.1346,  0.3834,  0.5632],\n",
      "        [ 0.6371, -0.0706,  1.1372,  0.7815, -1.2001],\n",
      "        [ 0.5383,  0.5353, -0.2802, -0.9498, -1.0700],\n",
      "        [ 1.2053, -0.4515,  0.7759, -0.8358,  0.1974]]) \n",
      " tensor([[-0.5814, -0.0000, -0.1683,  0.0000,  0.7040],\n",
      "        [ 0.7964, -0.0883,  1.4215,  0.9769, -1.5001],\n",
      "        [ 0.6728,  0.6692, -0.0000, -1.1872, -1.3375],\n",
      "        [ 0.0000, -0.0000,  0.9699, -1.0447,  0.2467]]) \n",
      " tensor([[-0.5814, -1.9601, -0.1683,  0.4793,  0.7040],\n",
      "        [ 0.7964, -0.0883,  1.4215,  0.9769, -1.5001],\n",
      "        [ 0.6728,  0.6692, -0.3502, -1.1872, -1.3375],\n",
      "        [ 1.5066, -0.5644,  0.9699, -1.0447,  0.2467]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input_1 = torch.randn(4, 5)\n",
    "output_1 = m(input_1)\n",
    "output_2 = input_1 * (1 / (1 - 0.2))\n",
    "print(input_1,'\\n',output_1,'\\n',output_2) #注意output_1中不是0的数字都和output_2一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单行条件语句\n",
    "\n",
    "**格式：**`[on_true] if [expression] else [on_false]`\n",
    "\n",
    "**例如：**\n",
    "```python\n",
    "x = \"Success!\" if (y==2) else \"Failed!\"\n",
    "```\n",
    "------\n",
    "**也可以多个判断：**\n",
    "```python\n",
    "x = int(input())\n",
    "if x >= 10:\n",
    "    print(\"horse\")\n",
    "elif 1 < x < 10:\n",
    "    print(\"Duck\")\n",
    "else:\n",
    "    print(\"other\")\n",
    "```\n",
    "上面的代码一行可以写完：\n",
    "\n",
    "`print('horse' if x >= 10 else \"Duck\" if 1 < x < 10 else \"other\")`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assert函数\n",
    "\n",
    "* 格式：assert expression [, arguments]\n",
    "\n",
    "```python\n",
    ">>> assert 1==2, '1 不等于 2'\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "AssertionError: 1 不等于 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\*args和\\**kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 (7, 8, 9) {'a': 1, 'b': 2, 'c': 3}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " *args 用来将参数打包成tuple给函数体调用\n",
    " **kwargs 打包关键字参数成dict给函数体调用\n",
    "'''\n",
    "def function(arg,*args,**kwargs):\n",
    "    print(arg,args,kwargs)\n",
    "\n",
    "function(6,7,8,9,a=1, b=2, c=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enumerate(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 j\n",
      "1 i\n",
      "2 a\n",
      "3 n\n"
     ]
    }
   ],
   "source": [
    "surname = \"jian\" \n",
    "for position_index, character in enumerate(surname):\n",
    "    print(position_index,character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同的tpye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同type之间的转换\n",
    "\n",
    "- **Tensor与Numpy Array之间的转换：**\n",
    "\n",
    "  Tensor----> Numpy  可以使用 data.numpy()，data为Tensor变量\n",
    "\n",
    "  Numpy ----> Tensor 可以使用 torch.from_numpy(data)，data为numpy变量\n",
    "  \n",
    "  \n",
    "\n",
    "- **List类型与numpy.array类型的互相转换：**\n",
    "\n",
    "  temp = np.array(list) \n",
    "\n",
    "  arr = temp.tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list类（一些字符串型在“文本处理”部分）\n",
    "\n",
    "### 列表表达式\n",
    "```python\n",
    "[ expression for item in list if conditional ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mylist= [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \n",
      " squares= [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] \n",
      " my_formula= [2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0] \n",
      "filtered= [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    }
   ],
   "source": [
    "mylist = [i for i in range(10)]\n",
    "squares = [x**2 for x in range(10)]\n",
    "def my_function(a):\n",
    "    return (a + 5) / 2\n",
    "my_formula = [my_function(x) for x in range(10)]\n",
    "filtered = [x for x in range(20) if x%2==0]\n",
    "print('mylist=',mylist,'\\n',\n",
    "     'squares=',squares,'\\n',\n",
    "     'my_formula=',my_formula,'\\n'\n",
    "      'filtered=',filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list切片\n",
    "```python\n",
    "a[start:stop:step]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rev_string= gfedcba \n",
      "\r",
      " rev_array= [5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "rev_string = \"abcdefg\"[::-1]\n",
    "rev_array = [1,2,3,4,5][::-1]\n",
    "print(\"rev_string=\",rev_string,\"\\n\\r\",\n",
    "     \"rev_array=\",rev_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计函数：set(list),max(list),list.count,Counter(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5, 6}\n"
     ]
    }
   ],
   "source": [
    "list_test = [1,1,2,3,4,5,5,5,6,6]\n",
    "print(set(list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list.count是list的内置函数\n",
    "list_test.count(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_test.count(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({5: 3, 1: 2, 6: 2, 2: 1, 3: 1, 4: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(list_test) # 显然可以用在str上统计字符个数\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map()函数\n",
    "\n",
    "* 语法：\n",
    "  `map(function, iterable, ...)`\n",
    "* 描述：第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表；\n",
    "* 参数：\n",
    "    - function --> 函数\n",
    "    - iterable --> 一个或多个序列\n",
    "* 返回值：迭代器\n",
    "* 实例：\n",
    "\n",
    "```python\n",
    ">>>def square(x) :            # 计算平方数\n",
    "...     return x ** 2\n",
    "\n",
    ">>> map(square, [1,2,3,4,5])   # 计算列表各个元素的平方\n",
    "out:[1, 4, 9, 16, 25]\n",
    "```\n",
    "------\n",
    "\n",
    "```python\n",
    ">>> map(lambda x: x ** 2, [1, 2, 3, 4, 5])  # 使用 lambda 匿名函数\n",
    "out:[1, 4, 9, 16, 25]\n",
    " \n",
    "# 提供了两个列表，对相同位置的列表数据进行相加\n",
    ">>> map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])\n",
    "out:[3, 7, 11, 15, 19]\n",
    "\n",
    ">>> def upper(s):\n",
    "    return s.upper()\n",
    "\n",
    ">>> mylist = list(map(upper,['sentence','fragment']))\n",
    ">>> print(mylist)\n",
    "out:['SENTENCE','FRAGMENT']\n",
    "\n",
    "# Convert a string representation of a number into a list of ints.\n",
    ">>> list_of_ints = list(map(int,\"123456\")))\n",
    ">>> print(list_of_ints)\n",
    "out: [1,2,3,4,5,6]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip()函数\n",
    "\n",
    "```python\n",
    ">>> a = [1,2,3]\n",
    ">>> b = [4,5,6]\n",
    ">>> c = [4,5,6,7,8]\n",
    ">>> zipped = zip(a,b) # 返回一个对象\n",
    ">>> zipped\n",
    "<zip object at 0x103abc288>\n",
    ">>> list(zipped)  # list() 转换为列表\n",
    "[(1, 4), (2, 5), (3, 6)]\n",
    ">>> list(zip(a,c))  # 元素个数与最短的列表一致\n",
    "[(1, 4), (2, 5), (3, 6)]\n",
    " \n",
    ">>> a1, a2 = zip(*zip(a,b)) # 与 zip 相反，zip(*) 可理解为解压，返回二维矩阵式\n",
    ">>> list(a1)\n",
    "[1, 2, 3]\n",
    ">>> list(a2)\n",
    "[4, 5, 6]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据类（python3.7开始支持）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card.rank= Q \n",
      " \n",
      " card= Card(rank='Q', suit='hearts')\n"
     ]
    }
   ],
   "source": [
    "# 具体详见：https://realpython.com/python-data-classes/\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Card:\n",
    "    rank:str\n",
    "    suit:str\n",
    "        \n",
    "card = Card(\"Q\",\"hearts\")\n",
    "\n",
    "print('card.rank=',card.rank,'\\n',\n",
    "      '\\n',\n",
    "     'card=',card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dict类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google www.google.com\n",
      "taobao www.taobao.com\n",
      "Runoob www.runoob.com\n"
     ]
    }
   ],
   "source": [
    "my_dict =  {'Google': 'www.google.com', 'taobao': 'www.taobao.com', 'Runoob': 'www.runoob.com'}\n",
    "for key,value in my_dict.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 3, 'c': 4}\n"
     ]
    }
   ],
   "source": [
    "dict1 = {'a':1,'b':2}\n",
    "dict2 = {'b':3,'c':4}\n",
    "merged = { **dict1,**dict2 } # 如果有重复的key，那么第一个词典的这个key对应的value会被覆盖掉\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些模型细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## residual connection\n",
    "```python\n",
    "# 来自《The Annotated Transformer》by harvestnlp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text Data Augmentation \n",
    "来自<EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks> in 2019\n",
    "\n",
    "* three augmentation strategies: \n",
    "  - random insertion\n",
    "  - random swap\n",
    "  - random deletion\n",
    "  \n",
    "> The techniques in the EDA paper average about a 3% improvement in accuracy when used with small amounts of labeled examples (roughly 500). If you have more than 5,000 examples in your dataset, the paper suggests that this improvement may fall to 0.8% or lower, due to the model obtaining better generalization from the larger amounts of data available over the improvements that EDA can provide.\n",
    "\n",
    "* Back Translation\n",
    "\n",
    "```python\n",
    "pip install googletrans\n",
    "# Then, we can translate our sentence from English to French, and then back to English\n",
    "import googletrans\n",
    "import googletrans.Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "sentences = ['The cat sat on the mat']\n",
    "\n",
    "translation_fr = translator.translate(sentences, dest='fr')\n",
    "fr_text = [t.text for t in translations_fr]\n",
    "translation_en = translator.translate(fr_text, dest='en')\n",
    "en_text = [t.text for t in translation_en]\n",
    "print(en_text)\n",
    "\n",
    "[out]:['The cat sat on the carpet']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232.604px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
